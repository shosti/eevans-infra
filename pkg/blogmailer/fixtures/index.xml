<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Emanuel Evans</title><link>https://eevans.co/</link><description>Recent content on Emanuel Evans</description><generator>Hugo -- gohugo.io</generator><language>en</language><lastBuildDate>Sun, 13 Aug 2023 00:00:00 +0000</lastBuildDate><atom:link href="https://eevans.co/index.xml" rel="self" type="application/rss+xml"/><item><title>Book Review: The Theoretical Minimum</title><link>https://eevans.co/blog/theoretical-minimum-review/</link><pubDate>Sun, 13 Aug 2023 00:00:00 +0000</pubDate><guid>https://eevans.co/blog/theoretical-minimum-review/</guid><category>book reviews</category><category>science</category><description>
&lt;p>
&lt;figure>
&lt;img src="https://eevans.co/blog/theoretical-minimum-review/theoretical-minimum_huc2601fa88bdc5f98424b737fc138ee58_58327_350x0_resize_q75_box.jpg" height="525" width="350" />
&lt;/figure>
&lt;/p>
&lt;p>
It&amp;#39;s hard to imagine a book that seems more tailor-made for me than &lt;em>The
Theoretical Minimum&lt;/em>. I feel like I have a decent understand of physics at a pop
science level but not at a mathematical level (especially when it comes to
quantum mechanics), and I&amp;#39;m often frustrated by the lack of depth in general
science books. From a math perspective, I know some calculus but have forgotten
a lot of the details. The idea of &lt;em>The Theoretical Minimum&lt;/em> is to teach the
basics of theoretical physics—specifically classical mechanics—to people in that
exact boat.&lt;/p>
&lt;p>
Does it work? Kind of! I don&amp;#39;t necessarily feel like I can start &amp;#34;doing physics&amp;#34;
now (which is the promise of the title), but I do have a better grasp of what
classical mechanics is all about, at least from a mathematical
perspective. (Other books in the series cover quantum mechanics and relativity,
and I&amp;#39;m looking forward to reading them.)&lt;/p>
&lt;p>
The emphasis here is very much on the &lt;em>theoretical&lt;/em>: there&amp;#39;s a lot of math and
few real-world examples. The most complex physical system described is a double
pendulum, and there&amp;#39;s barely a mention of cumbersome real-world phenomena like
friction. Instead, it&amp;#39;s basically a whirlwind tour of the Newtonian, Lagrangian,
and Hamiltonian formulations of classical mechanics.&lt;/p>
&lt;div id="outline-container-headline-1" class="outline-2">
&lt;h2 id="headline-1">
Newtonian Mechanics
&lt;/h2>
&lt;div id="outline-text-headline-1" class="outline-text-2">
&lt;p>After introducing the basic ideas of states in physics providing some quick
calculus refreshers, the book dives into variations of the familiar Newton
equations of motion (in a nutshell: &amp;#34;How many ways can you say $F=ma$?&amp;#34;) along
with related ideas like phase space and energy conservation. What I found most
challenging about this section (which was a continuing theme throughout the
book) was the mathematical notation—physicists seem to have a penchant for
extreme notational brevity and a lot of implicit convention, leading to
elegant-but-cryptic formulas like:&lt;/p>
&lt;p>
$$
\displaylines{\dot{p_i}=F_i\{(x)\}. \\ \dot{r_i}=\frac{p_i}{m}.}
$$&lt;/p>
&lt;p>
That&amp;#39;s kind of hard to interpret if you&amp;#39;re a physics newbie! (Let me see if I
can remember well enough to try: it&amp;#39;s something like &amp;#34;the time-derivative of the
momentum of a particle equals the force of a particle given the state of all
particles in the system; the time-derivative of the position of a particle
equals its momentum divided by its mass.&amp;#34;) I&amp;#39;d have appreciated a more gradual
introduction to the notational conventions, but I guess it&amp;#39;s all part of the
learning curve for understanding how physicists do their thing.&lt;/p>
&lt;p>
Despite the notational struggles, I found the basic concepts through Lecture 5
to be mostly intuitive (thanks to vague recollections of high-school-level
physics).&lt;/p>
&lt;/div>
&lt;/div>
&lt;div id="outline-container-headline-2" class="outline-2">
&lt;h2 id="headline-2">
Lagrangian Mechanics
&lt;/h2>
&lt;div id="outline-text-headline-2" class="outline-text-2">
&lt;p>Things got more interesting (for me) with the introduction of the Principal of
Least Action and Lagrangian Mechanics. The fundamental formulas involved are the
definition of Action:&lt;/p>
&lt;p>
$$
\mathcal{A} = \int_{t_0}^{t_1} L(q,\dot{q})dt
$$&lt;/p>
&lt;p>
And the Principal of Least Action:&lt;/p>
&lt;p>
$$
\delta \mathcal{A} = \delta \int_{t_0}^{t_1} L(q,\dot{q})dt = 0
$$&lt;/p>
&lt;p>
Which leads to the Euler-Lagrange Equation:&lt;/p>
&lt;p>
$$
\frac{d}{dt}\frac{\partial L}{\partial \dot{q}} - \frac{\partial L}{\partial q} = 0
$$&lt;/p>
&lt;p>
(Where $q$ means &amp;#34;a coordinate of any kind&amp;#34;.)&lt;/p>
&lt;p>
There&amp;#39;s a lot to unpack with those equations, starting with a basic question:
what is $L$? It&amp;#39;s called a Lagrangian, it&amp;#39;s a function related to a physical
system, and it&amp;#39;s &lt;em>often&lt;/em> equal to &amp;#34;kinetic energy minus potential energy&amp;#34;
(although not always). But how would one find a Lagrangian in the real world?
Unfortunately I&amp;#39;m not really sure—the examples in the book kind of assume you
have one lying around.&lt;/p>
&lt;p>
But once you have a Lagrangian and initial conditions, life is good. You can
determine the state of the system at any point of time through Euler-Lagrange
equations, and you can translate the system specification into different
coordinate systems without too much trouble (which, if I understand right, is
the biggest advantage over Newtonian formulations).&lt;/p>
&lt;p>
What does the Principal of Least Action actually mean? The book&amp;#39;s discussion was
a little bit hard for me to parse, but if I&amp;#39;m understanding right it&amp;#39;s something
like &amp;#34;changes in configuration follow a path that always minimizes changes&amp;#34;,
which is a pretty mind blowing property of the universe. (Side note: Wikipedia
informs me that the rate of change is not always minimal, just stationary.) I&amp;#39;m
hoping there&amp;#39;s more discussion in sequels to &lt;em>The Theoretical Minimum&lt;/em> because
I&amp;#39;m not sure I fully understand the implications yet.&lt;/p>
&lt;/div>
&lt;/div>
&lt;div id="outline-container-headline-3" class="outline-2">
&lt;h2 id="headline-3">
Hamiltonian Mechanics
&lt;/h2>
&lt;div id="outline-text-headline-3" class="outline-text-2">
&lt;p>After a few chapters in Lagrange-land, the discussion switches over to
Hamiltonian Mechanics, which is based on Hamilton&amp;#39;s Equations:&lt;/p>
&lt;p>
$$
\displaylines{\frac{\partial H}{\partial p_i} = \dot{q_i}. \\ \frac{\partial H}{\partial q_i} = -\dot{p_i}.}
$$&lt;/p>
&lt;p>
Where $H$ is another function (the Hamiltonian) defined in terms of the
Lagrangian:&lt;/p>
&lt;p>
$$
H = \sum_{i}(p_i \dot{q_i}) - L
$$&lt;/p>
&lt;p>
The big change is that instead of dealing with configuration space (basically
&amp;#34;positions of things&amp;#34;), Hamiltonian Mechanics deals with phase space (where
&amp;#34;positions of things&amp;#34; and &amp;#34;momenta of things&amp;#34; are all variables). (In typical
Physics Notation fashion, you just have to know that $q$ generally means a
coordinate and $p$ generally means a momentum to understand the equations.) The
upshot is that if you know the positions and momenta of everything in a system
at a point in time, plus the Hamiltonian, you can predict the future of the
system forever (using only first-order differential equations).&lt;/p>
&lt;p>
The book doesn&amp;#39;t have much discussion of the big-picture differences between
Lagrangian and Hamiltonian Mechanics (like when you would want to use one or the
other), although it does allude to the fact that Hamiltonian Mechanics becomes
very important in quantum mechanics.&lt;/p>
&lt;/div>
&lt;/div>
&lt;div id="outline-container-headline-4" class="outline-2">
&lt;h2 id="headline-4">
The Rest
&lt;/h2>
&lt;div id="outline-text-headline-4" class="outline-text-2">
&lt;p>After introducing Hamiltonian Mechanics, the book moves onto a grab bag of
related topics. Lecture 9 uses the Hamiltonian to discuss Liouville&amp;#39;s Theorem,
which states &amp;#34;the phase space fluid is incompressible&amp;#34;. This was the one section
that lost me entirely—I just don&amp;#39;t understand what any of that means from a
real-world perspective, or what the mathematical implications are.&lt;/p>
&lt;p>
Then the book introduces the &lt;a href="https://en.wikipedia.org/wiki/Poisson_bracket">Poisson bracket&lt;/a> notational system, which I actually
really like (since it comes with a nice set of axioms for algebraic
manipulation). After that, there&amp;#39;s some (very fast) discussion of angular
momentum and electricity and magnetism, but I&amp;#39;ll definitely need more practice
to feel comfortable with either of those topics.&lt;/p>
&lt;/div>
&lt;/div>
&lt;div id="outline-container-headline-5" class="outline-2">
&lt;h2 id="headline-5">
A Note on the Math
&lt;/h2>
&lt;div id="outline-text-headline-5" class="outline-text-2">
&lt;p>The book includes a fair number of exercises, which is nice because there&amp;#39;s a
lot of math that&amp;#39;s easy to almost-understand but hard to internalize. The
exercises ranged in difficulty from &amp;#34;trivial&amp;#34; to &amp;#34;way beyond my (limited) math
abilities&amp;#34;, but your mileage will vary based on how faded your memory of
calculus is. Thankfully there are quite a few sets of solutions online (I found
&lt;a href="https://tales.mbivert.com/on-the-theoretical-minimum-solutions/">these ones&lt;/a> to be particularly helpful).&lt;/p>
&lt;p>
In terms of the math background you need to get through &lt;em>The Theoretical
Minimum&lt;/em>, if you&amp;#39;ve never taken Calculus III you&amp;#39;re gonna have a bad time—there
are partial derivatives on basically every page. Some background knowledge on
differential equations is also useful, although thankfully none of the exercises
require solving non-trivial differential equations.&lt;/p>
&lt;p>
(It&amp;#39;s quite funny to me that this book was a bestseller—I can&amp;#39;t help but wonder
how many of the people who bought it actually made it all the way through.)&lt;/p>
&lt;/div>
&lt;/div>
&lt;div id="outline-container-headline-6" class="outline-2">
&lt;h2 id="headline-6">
The Upshot
&lt;/h2>
&lt;div id="outline-text-headline-6" class="outline-text-2">
&lt;p>&lt;em>The Theoretical Minimum&lt;/em> takes on an extremely ambitious—some might say
impossible—challenge: introduce technically-minded readers with minimal physics
background to classical mechanics in about 200 pages. Despite a few gripes, I&amp;#39;d
say it rises to the challenge pretty well. (The &lt;a href="https://www.youtube.com/playlist?list=PLq1A-pAPin7Vlo_KP_tJPJQMZq50nTE0G">companion lectures&lt;/a> by Leonard
Susskind were helpful for me when I got stuck, since they cover the concepts
more slowly.)&lt;/p>
&lt;p>
If I have one overarching complaint, it&amp;#39;s that I wish the authors spent a &lt;em>bit&lt;/em>
more time discussing &amp;#34;big-picture&amp;#34; ideas before diving into the math. I now
understand how one would &lt;em>use&lt;/em> a Lagrangian or a Hamiltonian, but I&amp;#39;m not quite
sure I have a clear picture of what either of them really &lt;em>means&lt;/em> in relation to
the physical universe or how one would find them through experiment. (Oh and a
side-complaint: a few more practical examples would have helped drill in the
concepts.)&lt;/p>
&lt;p>
Overall, &lt;em>The Theoretical Minimum&lt;/em> is a great jumping-off point for a longer
amateur physics journey. I fully intend to read the other books in the series
(I&amp;#39;ve already started &lt;em>Quantum Mechanics&lt;/em>), and if I have time in the future I
hope to dive deeper into classical mechanics (&lt;a href="https://mitp-content-server.mit.edu/books/content/sectbyfn/books_pres_0/9579/sicm_edition_2.zip/book.html">The Structure and Interpretation
of Classical Mechanics&lt;/a> looks promising).&lt;/p>
&lt;/div>
&lt;/div></description></item><item><title>Book Review: The Bright Ages</title><link>https://eevans.co/blog/bright-ages-review/</link><pubDate>Sun, 16 Jul 2023 00:00:00 +0000</pubDate><guid>https://eevans.co/blog/bright-ages-review/</guid><category>history</category><category>book reviews</category><description>
&lt;p>
(Note to readers: I haven&amp;#39;t had much time for writing technical deep-dives
lately, but I thought I&amp;#39;d publish some book reviews to keep the blog alive.)&lt;/p>
&lt;p>
&lt;figure>
&lt;img src="https://eevans.co/blog/bright-ages-review/brightages_hub38b434eed303bab6ba87c7ea46d1ab1_53807_0x300_resize_q75_box.jpg" height="300" width="199" />
&lt;figcaption>The Bright Ages&lt;/figcaption>
&lt;/figure>
&lt;/p>
&lt;p>
The general gist behind &lt;em>The Bright Ages&lt;/em> is something like: &amp;#34;Medieval Europe
wasn&amp;#39;t as bad as you think.&amp;#34; Which begs the question: how bad do &amp;#34;you&amp;#34; (I guess
&amp;#34;I&amp;#34; in this situation) think things were in the Middle Ages?&lt;/p>
&lt;p>
I&amp;#39;ve read a few books that cover medieval history (and watched the best
documentary on the period, &lt;em>Game of Thrones&lt;/em>), and my preconceptions of Medieval
Europe before reading &lt;em>The Bright Ages&lt;/em> were something like:&lt;/p>
&lt;ul>
&lt;li>Life was generally unpleasant and dangerous for the vast majority of people,
especially those who weren&amp;#39;t part of the nobility (although I didn&amp;#39;t have a
great sense of whether things got better or worse for the &amp;#34;average Joe&amp;#34; after
the fall of Rome or how much they improved after the Renaissance).&lt;/li>
&lt;li>Western Europe (and specifically Western Europe, not the surrounding world)
took a big step backwards in terms of math, philosophy, and engineering after
the fall of the Roman Empire (but caught up during the Renaissance).&lt;/li>
&lt;li>The Byzantine Empire was weird—it was a continuation of Roman society in a lot
of ways, but it&amp;#39;s hard to get a sense of how it compared to its neighbors in
terms of technology or political power.&lt;/li>
&lt;li>In general, Medieval Western Europe just wasn&amp;#39;t a particularly relevant place
in the grand scheme of things, at least compared to various Islamic and Asian
empires of the time.&lt;/li>
&lt;/ul>
&lt;p>After reading &lt;em>The Bright Ages&lt;/em>–well, I can&amp;#39;t say any of my impressions have
changed, mostly because the authors didn&amp;#39;t seem all that interested in those
topics. As with any revisionist history, the authors are arguing against
&lt;em>something&lt;/em>, but sometimes it&amp;#39;s hard to know &lt;em>what&lt;/em> unless you&amp;#39;re immersed in
specific academic squabbles (which I definitely am not in this case).&lt;/p>
&lt;p>
From my best inference, they&amp;#39;re trying to debunk ideas like:&lt;/p>
&lt;ul>
&lt;li>The Fall of Rome was a cataclysmic collapse where barbarians instantly overran
&amp;#34;civilization&amp;#34;.&lt;/li>
&lt;li>There was very little worthwhile art created during the Middle Ages.&lt;/li>
&lt;li>Women and religious minorities had less power and/or were treated worse than
in other time periods.&lt;/li>
&lt;li>Europeans of the time were insular and small-minded and didn&amp;#39;t know anything
about the outside world.&lt;/li>
&lt;/ul>
&lt;p>I didn&amp;#39;t really have those preconceptions before reading the book, and most of
their arguments either felt obvious or fell flat. Still, if you ignore the
overall premise, &lt;em>The Bright Ages&lt;/em> was an entertaining and informative (if
somewhat lightweight) general history of the time period.&lt;/p>
&lt;p>
Digging into some specific themes:&lt;/p>
&lt;div id="outline-container-headline-1" class="outline-2">
&lt;h2 id="headline-1">
Medieval Art—Was It All Worthless Junk?
&lt;/h2>
&lt;div id="outline-text-headline-1" class="outline-text-2">
&lt;p>Probably not! I never took an Art History class in college, so I think I&amp;#39;ve been
spared some clichés about how the artists of the Italian Renaissance took a
giant leap forward from the primitive scribblings of their medieval
forebears. But from what I gather, variations on that theme are fairly
commonly-held notions in the academic art world.&lt;/p>
&lt;p>
The authors clearly take issue with that view, extolling the virtues of medieval
artworks in Ravenna, Norway, Paris, and Toledo, and they don&amp;#39;t have to work hard
to convince me! I&amp;#39;m in no way an Art History buff, but it seems strange that
there&amp;#39;s even a debate about how &amp;#34;good&amp;#34; medieval art is—I thought most people
consider Notre-Dame to be one of Europe&amp;#39;s greatest masterpieces? Reading
between the lines, I&amp;#39;d guess the trashing of medieval art (which started in the
Renaissance and has seemingly gone on ever since) was probably the biggest
motivation behind &lt;em>The Bright Ages&lt;/em>.&lt;/p>
&lt;p>
The defense of medieval art was the most successful aspect of the book for me,
but maybe that&amp;#39;s because I had few preconceptions on my way in (other than
generally enjoying Gothic architecture as a tourist).&lt;/p>
&lt;/div>
&lt;/div>
&lt;div id="outline-container-headline-2" class="outline-2">
&lt;h2 id="headline-2">
Did Rome Actually Fall?
&lt;/h2>
&lt;div id="outline-text-headline-2" class="outline-text-2">
&lt;p>On the seventeenth page of the introduction the authors drop this bombshell:&lt;/p>
&lt;blockquote>
&lt;p>We&amp;#39;re going to start by following the travels, wiles, victories, and tragedies
of Galla Placidia to offer a simple reframing of the fifth century under one
premise: Rome did not fall.&lt;/p>
&lt;/blockquote>
&lt;p>
A spicy take! I thought maybe the book would end up &amp;#34;reframing the Middle Ages
from the perspective of Byzantium/the Eastern Roman Empire&amp;#34; or something edgy
like that. (I&amp;#39;d actually like to read that book!)&lt;/p>
&lt;p>
But alas, their actual arguments were pretty underwhelming: there was &lt;em>some&lt;/em>
communication between Byzantium and Rome; the Goths who sacked Rome wanted to
call themselves &amp;#34;Roman&amp;#34;; the Pope was a source of continuity to the Western
Roman Empire; Charlemagne ended up taking the title of Roman Emperor;
etc. Summarized:&lt;/p>
&lt;blockquote>
&lt;p>We have to remember that Rome as &amp;#34;empire&amp;#34; changed, but it had always been
changing. Change was part of the story from the very beginning. Its centers of
power moved. Its spheres of influence fragmented, coalesced, and fragmented
again. The idea that Rome &amp;#34;fell,&amp;#34; on the other hand, relies upon a conception of
homogeneity—of historical stasis.&lt;/p>
&lt;/blockquote>
&lt;p>
This is a bit like saying &amp;#34;The Beatles never really broke up because tribute
bands still exist to this day.&amp;#34; I&amp;#39;m no expert, but it seems obvious that the
Roman Empire of Constantine had a lot more in common with the Roman Empire of
Augustus then it did with the &amp;#34;Roman Empire&amp;#34; of Charlemagne (politically, at
least). Even the basic form of the argument is bizarre—did the Incan Empire just
&amp;#34;change and shift&amp;#34; when the Spanish conquered it?&lt;/p>
&lt;p>
(Brad DeLong has a much more thorough &lt;a href="https://braddelong.substack.com/p/yes-rome-did-fall">refutation&lt;/a> of the &amp;#34;continuous Roman
Empire&amp;#34; idea.)&lt;/p>
&lt;/div>
&lt;/div>
&lt;div id="outline-container-headline-3" class="outline-2">
&lt;h2 id="headline-3">
Were Things Really That Bad for Women?
&lt;/h2>
&lt;div id="outline-text-headline-3" class="outline-text-2">
&lt;p>The book really goes off the rails when the trying to defend the status of women
in the Middle Ages. The authors talk about how Vikings had relative parity
between sexes because there were Viking warriors who were women—exactly one
paragraph after saying that rape and sexual slavery were common, and ritualized
murder of girls also happened from time to time!&lt;/p>
&lt;p>
Elsewhere the authors discuss Eleanor of Aquitaine and Hildegard von Bingen, who
happen to be two of the only medieval women I&amp;#39;d heard of in advance. (I did
learn more about Hildegard von Bingen; I thought of her primarily as a composer
and didn&amp;#39;t realize she had some political influence as well.) Pointing to two
women with power over a thousand-year time span does not make a compelling case
that things were great for women overall.&lt;/p>
&lt;/div>
&lt;/div>
&lt;div id="outline-container-headline-4" class="outline-2">
&lt;h2 id="headline-4">
How About Everyone Else?
&lt;/h2>
&lt;div id="outline-text-headline-4" class="outline-text-2">
&lt;p>The authors barely even try to make the case that life wasn&amp;#39;t generally terrible
for most of the population, especially Jews and other religious minorities. To
the extent that they do, it&amp;#39;s often by pointing to relative tolerance under
Islamic rule, which just strengthened my preconceptions that Medieval Europe was
pretty backward compared to its neighbors.&lt;/p>
&lt;/div>
&lt;/div>
&lt;div id="outline-container-headline-5" class="outline-2">
&lt;h2 id="headline-5">
Roads Not Taken
&lt;/h2>
&lt;div id="outline-text-headline-5" class="outline-text-2">
&lt;p>Reading &lt;em>The Bright Ages&lt;/em>, I found myself craving more big picture comparisons
of Medieval Europe to other time periods and societies. It&amp;#39;s pretty obvious that
life was terrible for women and religious minorities in the Middle Ages, but was
it really worse than under the Roman Empire? Did things improve in the
Renaissance? I honestly have no idea. The authors drop a few hints that maybe
&amp;#34;the Renaissance was much worse than you think&amp;#34; (which might have been a more
interesting book), but there&amp;#39;s nothing like a coherent argument.&lt;/p>
&lt;p>
Which is to say nothing of economic output, mathematical knowledge, engineering
capabilities, or basically anything involving a number—there&amp;#39;s virtually no
discussion of that in the entire book. I&amp;#39;d really like to know the story behind
this, for instance:&lt;/p>
&lt;p>
&lt;blockquote class="twitter-tweet">&lt;p lang="en" dir="ltr">Three centuries before the Industrial Revolution, Europe was already the richest part of the world. (Source: Worldmapper.) &lt;a href="https://t.co/moB4qt23fI">pic.twitter.com/moB4qt23fI&lt;/a>&lt;/p>&amp;mdash; Paul Graham (@paulg) &lt;a href="https://twitter.com/paulg/status/1641361551675342849?ref_src=twsrc%5Etfw">March 30, 2023&lt;/a>&lt;/blockquote>
&lt;script async src="https://platform.twitter.com/widgets.js" charset="utf-8">&lt;/script>
&lt;/p>
&lt;p>
Was Europe before the age of colonialism more economically dynamic than I
thought? I guess I&amp;#39;ll have to read a different book to find out.&lt;/p>
&lt;/div>
&lt;/div>
&lt;div id="outline-container-headline-6" class="outline-2">
&lt;h2 id="headline-6">
Verdict
&lt;/h2>
&lt;div id="outline-text-headline-6" class="outline-text-2">
&lt;p>If you&amp;#39;re looking for a balanced big-picture history of the Middle Ages, this
definitely isn&amp;#39;t it! But it&amp;#39;s a well-written exposition of some of the &amp;#34;nicer
parts&amp;#34; of Medieval European history, along with semi-contrarian recountings of
classics like the Crusades and the Age of the Vikings, so still worth a read overall.&lt;/p>
&lt;/div>
&lt;/div></description></item><item><title>How I Host This Blog From My Garage</title><link>https://eevans.co/blog/garage/</link><pubDate>Mon, 06 Dec 2021 00:00:00 +0000</pubDate><guid>https://eevans.co/blog/garage/</guid><category>kubernetes</category><category>k8s</category><category>homelab</category><category>self-hosted</category><category>technical</category><description>
&lt;p>
Over the past few years I&amp;#39;ve been running some servers in my garage as a
Kubernetes &lt;a href="https://www.reddit.com/r/homelab/">homelab&lt;/a>. I really like homelabbing—having my own hardware lying
around is great for tinkering with the latest and greatest &amp;#34;cloud native&amp;#34;
technologies, and it just makes me feel kind of warm and fuzzy to host some of
my own tools instead of relying on external services. (For instance, I use &lt;a href="https://gitea.com/">Gitea&lt;/a>
to host all of my private git repositories.)&lt;/p>
&lt;p>
My homelab has gone through a number of iterations and has gotten increasingly
complex and &amp;#34;enterprisey&amp;#34; as I&amp;#39;ve piled on software I want to try but definitely
don&amp;#39;t need. But I&amp;#39;ve always been extremely hesitant to publicly expose anything
to the internet. It&amp;#39;s one thing to run a &lt;a href="https://jellyfin.org/">media server&lt;/a> that can be accessed over
&lt;a href="https://www.wireguard.com/">WireGuard&lt;/a>, but it&amp;#39;s another thing entirely to open up HTTP ports on a home
firewall—a few misconfigurations and your home network could be open to DDoS,
hacking, or worse!&lt;/p>
&lt;p>
A few weeks ago I decided to finally take the plunge and try hosting this blog
from my garage. If you&amp;#39;re reading this, it either means that it&amp;#39;s working or
that I&amp;#39;ve chickened out!&lt;/p>
&lt;p>
&lt;figure>
&lt;img src="https://eevans.co/blog/garage/badserver.jpg" />
&lt;figcaption>An old homelab iteration—it doesn&amp;#39;t look quite this bad nowadays&lt;/figcaption>
&lt;/figure>
&lt;/p>
&lt;div id="outline-container-headline-1" class="outline-2">
&lt;h2 id="headline-1">
The Cluster
&lt;/h2>
&lt;div id="outline-text-headline-1" class="outline-text-2">
&lt;p>My homelab cluster has three fairly low-powered x86 servers running &lt;a href="https://www.flatcar-linux.org/">Flatcar
Linux&lt;/a>, which is &amp;#34;just enough operating system&amp;#34; to get Kubernetes running on bare
metal. All three servers run the Kubernetes control plane with &lt;a href="https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/ha-topology/">high availability&lt;/a>
along with all my other &amp;#34;normal&amp;#34; workloads. (If I had a bunch more servers I&amp;#39;d
try to separate out the Kubernetes control plane, but it seems to be working
fine as is.)&lt;/p>
&lt;p>
A general tendency with homelabs is to run more &amp;#34;infrastructure-level&amp;#34; software
than actual applications, and mine is no exception. Here&amp;#39;s an incomplete list of
things I&amp;#39;m running to keep everything running:&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://cilium.io/">Cilium&lt;/a> for cluster networking—this gives me a &amp;#34;supercharged&amp;#34; network plugin
that does nifty things like encrypt all the traffic between nodes and provide
&lt;a href="https://docs.cilium.io/en/v1.10/policy/language/#layer-7-examples">layer 7 firewall rules&lt;/a> (more on that later).&lt;/li>
&lt;li>&lt;a href="https://metallb.org/">MetalLB&lt;/a> for load balancing—this provides Kubernetes &lt;code>LoadBalancer&lt;/code> services in
non-&amp;#34;cloudy&amp;#34; environments (like my garage).&lt;/li>
&lt;li>&lt;a href="https://kubernetes.github.io/ingress-nginx/">nginx&lt;/a> for ingress—every once in a while I think about trying a &lt;a href="https://docs.google.com/spreadsheets/d/191WWNpjJ2za6-nbG4ZoUMXMpUK8KlCIosvQB0f-oq3k/edit#gid=907731238">different
ingress controller&lt;/a>, but none of the free ones have been compelling enough to
lure me away from nginx.&lt;/li>
&lt;li>&lt;a href="https://rook.io/">Rook&lt;/a> for clustered storage—this gives me &lt;a href="https://ceph.com/en/">Ceph&lt;/a>-backed storage that can be
accessed through Kubernetes &lt;a href="https://kubernetes.io/docs/concepts/storage/persistent-volumes/">Persistent Volumes&lt;/a>. The volumes stay available
even when a server is down, and you can access the data in a few different
ways (including S3-style object storage). I&amp;#39;ve been extremely impressed with
Rook and Ceph, and it hasn&amp;#39;t been as hard to keep running as I feared it might
be.&lt;/li>
&lt;li>&lt;a href="https://prometheus.io/">Prometheus&lt;/a>/&lt;a href="https://grafana.com/">Grafana&lt;/a> for monitoring and alerting—this is basically table stakes
for Kubernetes clusters these days, but it&amp;#39;s a pretty great stack.&lt;/li>
&lt;li>&lt;a href="https://grafana.com/oss/loki/">Loki&lt;/a> for log aggregation—I switched to Loki recently after trying for ages to
figure out a decent ElasticSearch setup, and it&amp;#39;s just so much better. Loki is
pretty easy to get running, and you access your logs through Grafana along
with your metrics (I didn&amp;#39;t realize what a good idea this is until I tried
it).&lt;/li>
&lt;li>&lt;a href="https://goharbor.io/">Harbor&lt;/a> as a private container registry—I&amp;#39;ve had some trouble with this one but
it works well enough.&lt;/li>
&lt;li>&lt;a href="https://fluxcd.io/">Flux&lt;/a> for deploying everything with &lt;a href="https://www.gitops.tech/">GitOps&lt;/a> (I have somewhat mixed feelings
about GitOps in general, but Flux is refreshingly simple and does the job).&lt;/li>
&lt;/ul>
&lt;p>To stave off the inevitable haters: &lt;strong>I know this is over-engineered&lt;/strong>. That&amp;#39;s
kind of the point! By trying out &amp;#34;flashy&amp;#34; software in a low-stakes environment,
I can have some idea of how it will work in production-critical ones, which
helps me make better choices. (As an example, my attempts to run Istio in a
homelab setting have convinced me that it should be avoided in most
circumstances.)&lt;/p>
&lt;p>
But I&amp;#39;m getting sidetracked, back to how I host the blog.&lt;/p>
&lt;/div>
&lt;/div>
&lt;div id="outline-container-headline-2" class="outline-2">
&lt;h2 id="headline-2">
Making Some Stuff Public
&lt;/h2>
&lt;div id="outline-text-headline-2" class="outline-text-2">
&lt;p>Until a few weeks ago this all existed in my happy self-contained local network,
free from the terrors of the public internet. But I&amp;#39;ve thought for a while that
it&amp;#39;s silly to have all this infrastructure sitting around and still pay for
cloud hosting. (Well, if I&amp;#39;m being honest, the blog was only costing me a few
dollars a month on &lt;a href="https://cloud.google.com/storage/">GCS&lt;/a>, but it&amp;#39;s the principle dammit!)&lt;/p>
&lt;p>
If I were a less paranoid person, I would set up my home firewall to forward
ports 80 and 443 to my cluster ingress IP address, add some public DNS records,
and call it a day. But that would have two problems:&lt;/p>
&lt;ul>
&lt;li>My home internet connection doesn&amp;#39;t have a static IP address, so there&amp;#39;d be
&lt;a href="https://en.wikipedia.org/wiki/Dynamic_DNS">dynamic DNS&lt;/a> futzing involved (never fun, but not a deal breaker if occasional
downtime is OK).&lt;/li>
&lt;li>More importantly: if I didn&amp;#39;t do everything exactly right I could have ended
up exposing all my private services to the internet, which would be a
disaster! (I try to run all my self-hosted apps with proper auth and
encryption, but you never know…) And even if I did do everything right,
there&amp;#39;s always &lt;a href="https://en.wikipedia.org/wiki/Denial-of-service_attack">DDoS&lt;/a> to worry about.&lt;/li>
&lt;/ul>
&lt;p>I considered a few options (including some exotic ones like using a Raspberry Pi
as a kind of &lt;a href="https://www.barracuda.com/glossary/dmz-network">DMZ&lt;/a>), but the solution I landed on is to use a &lt;a href="https://developers.cloudflare.com/cloudflare-one/connections/connect-apps">Cloudflare Tunnel&lt;/a> (I
guess formerly known as Argo Tunnel? Cloudflare&amp;#39;s branding confuses me
sometimes). Which is kind of like an industrial-grade version of &lt;a href="https://ngrok.com/">ngrok&lt;/a>: you run
a local daemon called &lt;code>cloudflared&lt;/code> that connects to Cloudflare&amp;#39;s
infrastructure, and then you get a &amp;#34;magic URL&amp;#34; that forwards connections from
the internet through the daemon to your local network.&lt;/p>
&lt;p>
This approach solves a lot of problems at once: there&amp;#39;s no need to open up any
inbound firewall ports (hooray!); I don&amp;#39;t need to set up dynamic DNS records for
my home IP address (which would, among other things, have some bad privacy
implications); and I get Cloudflare&amp;#39;s DDoS protection and CDN features (which I
would have wanted anyways for the blog). And crazily enough, it&amp;#39;s all free!&lt;/p>
&lt;p>
Deploying &lt;code>cloudflared&lt;/code> on Kubernetes is pretty straightforward: I just use a
standard Kubernetes &lt;code>Deployment&lt;/code> with the &lt;a href="https://hub.docker.com/r/cloudflare/cloudflared">cloudflared Docker image&lt;/a> and two
replicas for redundancy. The &lt;code>cloudflared&lt;/code> pods have a basic configuration that
forwards all traffic to an ingress controller.&lt;/p>
&lt;p>
&lt;figure>
&lt;img src="https://eevans.co/blog/garage/traffic.svg" />
&lt;figcaption>The convoluted path your packets are following to get this content&lt;/figcaption>
&lt;/figure>
&lt;/p>
&lt;/div>
&lt;/div>
&lt;div id="outline-container-headline-3" class="outline-2">
&lt;h2 id="headline-3">
Adding a Healthy Sprinkle of Paranoia
&lt;/h2>
&lt;div id="outline-text-headline-3" class="outline-text-2">
&lt;p>Using a Cloudflare Tunnel solved the big issue of exposing my home network to
the internet, but it wasn&amp;#39;t quite enough to assuage my fears. I took a few steps
to lock things down even more.&lt;/p>
&lt;div id="outline-container-headline-4" class="outline-3">
&lt;h3 id="headline-4">
Deploying a Separate Ingress Controller
&lt;/h3>
&lt;div id="outline-text-headline-4" class="outline-text-3">
&lt;p>I don&amp;#39;t want traffic for the blog getting mixed up with traffic for my internal
apps, so I deployed a &lt;a href="https://kubernetes.github.io/ingress-nginx/user-guide/multiple-ingress/">separate instance of ingress-nginx&lt;/a> in a new
namespace. This is pretty easy to manage thanks to Kubernetes &lt;a href="https://kubernetes.io/docs/concepts/services-networking/ingress/#ingress-class">ingress classes&lt;/a>:
for public apps, I can create &lt;code>Ingress&lt;/code> resources with &lt;code>ingressClassName:
nginx-public&lt;/code> and everything works out OK.&lt;/p>
&lt;p>
Nothing earth-shattering here, but worth mentioning since it&amp;#39;s a good idea for
any Kubernetes cluster that handles both private and public traffic.&lt;/p>
&lt;/div>
&lt;/div>
&lt;div id="outline-container-headline-5" class="outline-3">
&lt;h3 id="headline-5">
Locking Down Pods
&lt;/h3>
&lt;div id="outline-text-headline-5" class="outline-text-3">
&lt;p>Again somewhat boring, but I try to follow all the standard &amp;#34;security best
practices&amp;#34; for pods that could get public traffic, like making sure they all
have &lt;a href="https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/">resource requests/limits&lt;/a> and adding a &lt;a href="https://kubernetes.io/docs/tasks/configure-pod-container/security-context/">security context&lt;/a> like:&lt;/p>
&lt;div class="src src-yaml">
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>&lt;span style="font-weight:bold">securityContext&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="font-weight:bold">capabilities&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="font-weight:bold">drop&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - ALL
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="font-weight:bold">readOnlyRootFilesystem&lt;/span>: &lt;span style="color:#fff;font-weight:bold">true&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="font-weight:bold">runAsNonRoot&lt;/span>: &lt;span style="color:#fff;font-weight:bold">true&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="font-weight:bold">allowPrivilegeEscalation&lt;/span>: &lt;span style="color:#fff;font-weight:bold">false&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>
&lt;/div>
&lt;p>
I don&amp;#39;t really know much difference these things make (they&amp;#39;re basically handed
down from on high by The Security People), but I&amp;#39;m not a security expert so who
am I to argue.&lt;/p>
&lt;/div>
&lt;/div>
&lt;div id="outline-container-headline-6" class="outline-3">
&lt;h3 id="headline-6">
Locking Down Traffic
&lt;/h3>
&lt;div id="outline-text-headline-6" class="outline-text-3">
&lt;p>What I &lt;em>actually&lt;/em> care about from a security perspective is making sure that no
traffic from the internet can &lt;em>ever&lt;/em> reach anything private on my
network. Kubernetes has a tool for solving these sorts of problems: &lt;a href="https://kubernetes.io/docs/concepts/services-networking/network-policies/">network
policies&lt;/a> (which basically specify firewall rules for pods). The rules I wanted
for my &amp;#34;public&amp;#34; traffic flow were something like:&lt;/p>
&lt;ul>
&lt;li>&lt;code>cloudflared&lt;/code> is allowed to talk to Cloudflare and the nginx ingress
controller, but that&amp;#39;s it (since it&amp;#39;s more or less &amp;#34;untrusted&amp;#34;).&lt;/li>
&lt;li>
&lt;p>&lt;code>ingress-nginx&lt;/code> is allowed to talk to:&lt;/p>
&lt;ul>
&lt;li>The Kubernetes API (necessary since it&amp;#39;s an ingress controller).&lt;/li>
&lt;li>Any pods in the cluster with a special &lt;code>public&lt;/code> label, but no other pods
(that way even if I misconfigure an &lt;code>Ingress&lt;/code>, the public ingress controller
won&amp;#39;t be able to reach private pods).&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>Kubernetes network policies are pretty &amp;#34;dumb&amp;#34;: they allow you to filter traffic
based on IP addresses and pod/namespace labels. That was a problem for the
&lt;code>cloudflared&lt;/code> rules: I really wanted to allow access to DNS addresses (like
&lt;code>api.cloudflare.com&lt;/code>) instead of IP addresses (in network-speak: I wanted a
layer 7 policy instead of layer 4).&lt;/p>
&lt;p>
Cilium actually makes that possible through &lt;a href="https://docs.cilium.io/en/v1.10/concepts/kubernetes/policy/#ciliumnetworkpolicy">&lt;code>CliumNetworkPolicy&lt;/code>&lt;/a>s, which are
supercharged network policies that allow layer 7 filtering. Here&amp;#39;s what the
&lt;code>cloudflared&lt;/code> policy ended up looking like (slightly shortened):&lt;/p>
&lt;div class="src src-yaml">
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>&lt;span style="font-weight:bold">apiVersion&lt;/span>: cilium.io/v2
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="font-weight:bold">kind&lt;/span>: CiliumNetworkPolicy
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="font-weight:bold">metadata&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="font-weight:bold">name&lt;/span>: cloudflared-egress
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="font-weight:bold">namespace&lt;/span>: ingress-public
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="font-weight:bold">spec&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="font-weight:bold">endpointSelector&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="font-weight:bold">matchLabels&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="font-weight:bold">app.kubernetes.io/instance&lt;/span>: cloudflared
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="font-weight:bold">egress&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#007f7f"># ...&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#007f7f"># Allow tunnel connections to Cloudflare&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - &lt;span style="font-weight:bold">toFQDNs&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - &lt;span style="font-weight:bold">matchPattern&lt;/span>: &lt;span style="color:#0ff;font-weight:bold">&amp;#34;*.argotunnel.com&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="font-weight:bold">toPorts&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - &lt;span style="font-weight:bold">ports&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - &lt;span style="font-weight:bold">port&lt;/span>: &lt;span style="color:#0ff;font-weight:bold">&amp;#34;7844&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#007f7f"># Allow traffic to the Cloudflare API&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - &lt;span style="font-weight:bold">toFQDNs&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - &lt;span style="font-weight:bold">matchName&lt;/span>: api.cloudflare.com
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="font-weight:bold">toPorts&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - &lt;span style="font-weight:bold">ports&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - &lt;span style="font-weight:bold">port&lt;/span>: &lt;span style="color:#0ff;font-weight:bold">&amp;#34;443&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#007f7f"># Allow traffic to the ingress controller&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - &lt;span style="font-weight:bold">toEndpoints&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - &lt;span style="font-weight:bold">matchLabels&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="font-weight:bold">app.kubernetes.io/instance&lt;/span>: ingress-public
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="font-weight:bold">toPorts&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - &lt;span style="font-weight:bold">ports&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - &lt;span style="font-weight:bold">port&lt;/span>: &lt;span style="color:#0ff;font-weight:bold">&amp;#34;443&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - &lt;span style="font-weight:bold">ports&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - &lt;span style="font-weight:bold">port&lt;/span>: &lt;span style="color:#0ff;font-weight:bold">&amp;#34;80&amp;#34;&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>
&lt;/div>
&lt;p>
(There are similar policies for the ingress controller pods, but I don&amp;#39;t want to
inundate you with YAML so I&amp;#39;ll leave them out.) The policy files are a bit
verbose, but Cilium has a cool &lt;a href="https://editor.cilium.io/">GUI policy editor&lt;/a> that makes them easier to
generate.&lt;/p>
&lt;p>
&lt;figure>
&lt;img src="https://eevans.co/blog/garage/policy.png" />
&lt;figcaption>A visual representation of the cloudflared network policy&lt;/figcaption>
&lt;/figure>
&lt;/p>
&lt;p>
The upshot is that even if a &lt;code>cloudflared&lt;/code> pod were to be compromised somehow,
it would have very few opportunities to wreak havoc in my local network (at
least that&amp;#39;s what I&amp;#39;m hoping 🤞).&lt;/p>
&lt;/div>
&lt;/div>
&lt;/div>
&lt;/div>
&lt;div id="outline-container-headline-7" class="outline-2">
&lt;h2 id="headline-7">
The Actual Content
&lt;/h2>
&lt;div id="outline-text-headline-7" class="outline-text-2">
&lt;p>What about the actual website content? Here I ended up with a pretty boring
solution:&lt;/p>
&lt;ul>
&lt;li>Static HTML/CSS/etc. is generated with &lt;a href="https://gohugo.io/">Hugo&lt;/a> (which I was already using).&lt;/li>
&lt;li>I build an extremely simple Docker image based on an &lt;a href="https://hub.docker.com/r/nginxinc/nginx-unprivileged/">nginx image&lt;/a> with all the
static content copied in (along with a basic nginx configuration file).&lt;/li>
&lt;li>I deploy the blog image with a standard Kubernetes &lt;code>Deployment&lt;/code>.&lt;/li>
&lt;/ul>
&lt;p>So I use oodles of the latest and greatest in Cloud Native technologies
configured with thousands of lines of YAML to ultimately serve static HTML in a
way not too different from how any self-respecting webmaster would have done it
in the 90s. Hooray for technological progress.&lt;/p>
&lt;p>
At least performance seems pretty decent (round trip requests are about 50ms
uncached for me).&lt;/p>
&lt;/div>
&lt;/div>
&lt;div id="outline-container-headline-8" class="outline-2">
&lt;h2 id="headline-8">
Maybe Try This At Home
&lt;/h2>
&lt;div id="outline-text-headline-8" class="outline-text-2">
&lt;p>Well, hosting a blog from home is probably not a great idea from a practical
perspective. (I&amp;#39;ll see how this experiment works out…) But self hosting and
homelabbing in general can be a rewarding way to hone your DevOps/sysadmin
skills and maybe even get more control of your data, if you&amp;#39;re into that sort of
thing.&lt;/p>
&lt;p>
If you haven&amp;#39;t given homelabbing a try and you&amp;#39;re curious, here are some
resources that I&amp;#39;ve found helpful:&lt;/p>
&lt;ul>
&lt;li>The &lt;a href="https://www.reddit.com/r/homelab/">homelab&lt;/a> and &lt;a href="https://www.reddit.com/r/selfhosted/">SelfHosted&lt;/a> subreddits&lt;/li>
&lt;li>The &lt;a href="https://selfhosted.show/">Self Hosted&lt;/a> podcast&lt;/li>
&lt;li>&lt;a href="https://www.youtube.com/c/JeffGeerling">Jeff Geerling&amp;#39;s YouTube channel&lt;/a> (especially for Raspberry Pi craziness—if I
were starting over from scratch I&amp;#39;d probably go with Raspberry Pis to keep
power usage down)&lt;/li>
&lt;/ul>
&lt;p>If you like this stuff, &lt;a href="https://twitter.com/EmanuelMEvans">let me know&lt;/a> and I might do some more posts on homelab
adventures.&lt;/p>
&lt;/div>
&lt;/div></description></item><item><title>Implementing Raft for Browsers with Rust and WebRTC</title><link>https://eevans.co/blog/wraft/</link><pubDate>Tue, 30 Nov 2021 00:00:00 +0000</pubDate><guid>https://eevans.co/blog/wraft/</guid><category>rust</category><category>webassembly</category><category>wasm</category><category>webrtc</category><category>technical</category><description>
&lt;p>
I try to keep a vague list of &amp;#34;technologies to try out&amp;#34; handy at all
times. Usually things come and go from the list pretty quickly, but I&amp;#39;ve had a
few that have been stubbornly persistent for quite a while now:&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://www.rust-lang.org/">Rust&lt;/a> (at least for a big project)&lt;/li>
&lt;li>&lt;a href="https://raft.github.io/">The Raft algorithm&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://webassembly.org/">WebAssembly&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://webrtc.org/">WebRTC&lt;/a>&lt;/li>
&lt;/ul>
&lt;p>At some point, I had a bright idea: &amp;#34;Why not knock out a bunch of them at once?&amp;#34;
And thus &lt;a href="https://github.com/shosti/wraft">WRaft&lt;/a> was born. It&amp;#39;s a Raft implementation over WebRTC written in Rust
for WebAssembly (amazing buzzword compliance!).&lt;/p>
&lt;p>
There are a couple demo apps at &lt;a href="https://wraft0.eevans.co,">https://wraft0.eevans.co,&lt;/a> and the code is &lt;a href="https://github.com/shosti/wraft">open
source on GitHub&lt;/a>. It was a fun and challenging project to implement, so I
thought I&amp;#39;d write a blog post about the experience as a sort of &amp;#34;postmortem&amp;#34;.&lt;/p>
&lt;div id="outline-container-headline-1" class="outline-2">
&lt;h2 id="headline-1">
Dramatis Personae
&lt;/h2>
&lt;div id="outline-text-headline-1" class="outline-text-2">
&lt;p>Here&amp;#39;s an extremely quick summary of the technologies involved with the project:&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Rust&lt;/strong> is more or less the only mainstream language that&amp;#39;s memory-safe without
having a garbage collector (thanks to its unique &lt;a href="https://doc.rust-lang.org/book/ch04-00-understanding-ownership.html">ownership model&lt;/a> and borrow
checker). This allows it to thrive in situations where garbage-collected
languages struggle, including…&lt;/li>
&lt;li>&lt;strong>WebAssembly&lt;/strong> (or Wasm), which is a binary format for frontend web
applications (and, nowadays, other applications, although that&amp;#39;s a topic for
another blog post). The idea is that Wasm programs can run at near-native
speeds within a browser, which makes it an interesting choice for
implementing…&lt;/li>
&lt;li>&lt;strong>Raft&lt;/strong>, which is a &lt;a href="https://en.wikipedia.org/wiki/Consensus_(computer_science)">distributed consensus&lt;/a> algorithm. Raft safely and reliably
replicates data to multiple nodes (usually servers) and is the brains behind
&lt;a href="https://etcd.io/">etcd&lt;/a> and several other distributed databases. Of course, for nodes to
replicate data between each other they need to be able to communicate with
each other, which is a bit challenging for browser-based applications! But
possible thanks to…&lt;/li>
&lt;li>&lt;strong>WebRTC&lt;/strong>, which is a basically-peer-to-peer protocol that allows in-browser
programs to communicate with each other. The &amp;#34;real-world&amp;#34; use cases for WebRTC
generally involve video and audio sharing, but the protocol also includes &lt;a href="https://webrtc.org/getting-started/data-channels">data
channels&lt;/a> for transferring arbitrary data between web browsers, which is what I
used for WRaft.&lt;/li>
&lt;/ul>
&lt;/div>
&lt;/div>
&lt;div id="outline-container-headline-2" class="outline-2">
&lt;h2 id="headline-2">
The Project
&lt;/h2>
&lt;div id="outline-text-headline-2" class="outline-text-2">
&lt;p>My original idea was to build something like &amp;#34;etcd for browsers&amp;#34;, i.e. a
distributed peer-to-peer key-value store. Oddly enough, &lt;a href="https://raft.github.io/raft.pdf">the Raft paper&lt;/a> doesn&amp;#39;t
say anything about key-value stores or databases; instead it vaguely mentions a
&amp;#34;replicated state machine&amp;#34;. I ended up taking this pretty literally once the
project progressed; the end result is that WRaft will replicate &lt;em>any kind of
state machine&lt;/em> (specified by a &lt;a href="https://github.com/shosti/wraft/blob/main/wraft/src/raft/mod.rs#L312">Rust trait&lt;/a>) between browser windows.&lt;/p>
&lt;p>
This approach works great for React-style web applications, since they tend to
be built on top of state machines for data flow. As a demo, I took the &lt;a href="https://github.com/yewstack/yew/tree/master/examples/todomvc">Yew
TodoMVC example app&lt;/a> and &lt;a href="https://github.com/shosti/wraft/blob/main/wraft/src/todo_state.rs">replicated the entire app state&lt;/a> in WRaft. (&lt;a href="https://yew.rs/">Yew&lt;/a> is
something like a React/Redux clone for Rust/Wasm.) You can try out the result at
&lt;a href="https://wraft0.eevans.co/todo.">https://wraft0.eevans.co/todo.&lt;/a>&lt;/p>
&lt;p>
Diving into the overall architecture, a WRaft &amp;#34;cluster&amp;#34; looks something like
this:&lt;/p>
&lt;p>
&lt;figure>
&lt;img src="https://eevans.co/blog/wraft/introduced.svg" />
&lt;/figure>
&lt;/p>
&lt;p>
A cluster always has exactly three &amp;#34;nodes&amp;#34; (i.e. browser windows) in the current
implementation. The nodes find each other through a WebSocket-based introducer
service (more on that later), after which they start communicating over WebRTC
data channels. Each node stores data locally using &lt;a href="https://developer.mozilla.org/en-US/docs/Web/API/Web_Storage_API/Using_the_Web_Storage_API">local storage&lt;/a> (I experimented
a bit with &lt;a href="https://developer.mozilla.org/en-US/docs/Web/API/IndexedDB_API">IndexedDB&lt;/a> but it seemed prohibitively slow).&lt;/p>
&lt;p>
As long as two nodes stay online, the whole cluster is &amp;#34;healthy&amp;#34; and can
continue working; nodes that leave the cluster (for instance, if you reload the
page) will do their best to rejoin the cluster and catch up. Unfortunately, a
cluster can&amp;#39;t currently recover from all nodes coming down (I might try to fix
that at some point).&lt;/p>
&lt;p>
Cluster members can be on the same machine or different machines on a LAN
(theoretically it might even work across the internet, but I haven&amp;#39;t tested it
yet). Performance seems pretty decent: with a very basic &lt;a href="https://wraft0.eevans.co/bench">benchmark app&lt;/a> I&amp;#39;ve seen
over 2000 writes per second on Chromium, which is probably good enough for any
use case I can think of 😄.&lt;/p>
&lt;/div>
&lt;/div>
&lt;div id="outline-container-headline-3" class="outline-2">
&lt;h2 id="headline-3">
How The Project Went
&lt;/h2>
&lt;div id="outline-text-headline-3" class="outline-text-2">
&lt;p>It&amp;#39;s postmortem time! Here&amp;#39;s how things went; I hope some of this might be
useful for anyone treading similar paths in the future.&lt;/p>
&lt;div id="outline-container-headline-4" class="outline-3">
&lt;h3 id="headline-4">
Easier Than Expected: WebAssembly
&lt;/h3>
&lt;div id="outline-text-headline-4" class="outline-text-3">
&lt;p>I&amp;#39;ve done some &amp;#34;modern&amp;#34; frontend development in the recent past, and it&amp;#39;s
usually involved horrifically complicated setups with a ton of glue code to get
everything working together. Given that WebAssembly is pretty niche and
bleeding-edge, I feared the worst. But to my surprise, the WebAssembly setup
experience was impressively smooth. I was at &amp;#34;Hello World&amp;#34; using &lt;a href="https://rustwasm.github.io/wasm-pack/">wasm-pack&lt;/a> in
less time than it&amp;#39;s taken me to make an app with &lt;a href="https://create-react-app.dev/">create-react-app&lt;/a> in the past.&lt;/p>
&lt;p>
Even better, the &lt;a href="https://rustwasm.github.io/docs/wasm-bindgen/">wasm-bindgen&lt;/a> developers have done a fantastic job creating Rust
bindings for &lt;a href="https://rustwasm.github.io/wasm-bindgen/api/web_sys/">just about the entire web browser API space&lt;/a>, including the
relatively obscure APIs like WebRTC. You can even use Rust Futures with &lt;code>async&lt;/code>
/ &lt;code>await&lt;/code> syntax using the &lt;a href="https://docs.rs/wasm-bindgen-futures/0.4.28/wasm_bindgen_futures/">wasm_bindgen_futures&lt;/a> crate (which was pretty much
essential for implementing Raft). A surprising number of Rust crates work
flawlessly in WebAssembly (with notable exceptions like &lt;a href="https://tokio.rs/">Tokio&lt;/a>). I haven&amp;#39;t dug
into it yet, but JS↔Wasm interop also looks pretty straightforward.&lt;/p>
&lt;p>
The one tricky issue was dealing with &lt;a href="https://rustwasm.github.io/wasm-bindgen/examples/closures.html">JS closures&lt;/a>, which were pervasive in the
APIs I was using. The wasm-bindgen approach is quite awkward and one of the few
cases I ran across where you need to do a form of manual memory
management. (Apparently the &lt;a href="https://docs.rs/gloo/0.4.0/gloo/">gloo&lt;/a> crate makes it a little easier, but I didn&amp;#39;t
find out about it until I was done implementing all the hard JS-related parts.)&lt;/p>
&lt;/div>
&lt;/div>
&lt;div id="outline-container-headline-5" class="outline-3">
&lt;h3 id="headline-5">
Harder Than Expected: WebRTC
&lt;/h3>
&lt;div id="outline-text-headline-5" class="outline-text-3">
&lt;p>Implementing WebRTC communication, on the other hand, was much trickier than I
anticipated. I didn&amp;#39;t know much about WebRTC going into the project, but I had a
vague idea that there had to be some sort of server involved for NAT traversal
and whatnot.&lt;/p>
&lt;p>
What I didn&amp;#39;t realize is that WebRTC sessions need a &lt;a href="https://www.wowza.com/blog/webrtc-signaling-servers">signaling server&lt;/a> for
browser windows to exchange connectivity information before they can communicate
with each other. This might not be too bad, except for the fact that
&lt;em>WebRTC signaling servers have no standard protocol or popular ready-to-use implementations&lt;/em>.&lt;/p>
&lt;p>
Which means that if you want to use WebRTC, you&amp;#39;re going to have to implement
your own signaling server with some sort of ad-hoc protocol (or use a SaaS, but
where&amp;#39;s the fun in that?). WebSockets seems to be the most popular
implementation choice; the best documentation resources I could find were &lt;a href="https://developer.mozilla.org/en-US/docs/Web/API/WebRTC_API/Signaling_and_video_calling">an MDN
tutorial&lt;/a> and a &lt;a href="https://www.youtube.com/watch?v=Y1mx7cx6ckI">fairly old YouTube talk&lt;/a>. (Confusingly, signaling servers are
unrelated to NAT traversal, which relies on standardized protocols like &lt;a href="https://blog.ivrpowers.com/post/technologies/what-is-stun-turn-server/">STUN and
TURN&lt;/a>.)&lt;/p>
&lt;p>
And implementing the signaling server is only half the battle. There&amp;#39;s also the
arcane &lt;a href="https://developer.mozilla.org/en-US/docs/Web/API/WebRTC_API">WebRTC API&lt;/a> to deal with, with its myriad of events and callbacks that
arrive in hard-to-predict order and spout enormously unhelpful error messages if
you do anything wrong. (To be fair a lot of the complexity comes from
video/audio handling, but it&amp;#39;s pretty hard to avoid even for data-only
use-cases.)&lt;/p>
&lt;p>
Implementing the WebRTC-specific parts of WRaft probably ended up taking as much
time as everything else combined. The end result looks something like this:&lt;/p>
&lt;ul>
&lt;li>Every cluster gets a 128-bit &lt;strong>session key&lt;/strong> (I probably should have called this
a &amp;#34;cluster ID&amp;#34;, but oh well). The session key is randomly generated when the
first &amp;#34;node&amp;#34; (i.e. browser window) starts a cluster.&lt;/li>
&lt;li>Every node has a 64-bit &lt;strong>node id&lt;/strong>, which is basically &lt;code>Hash(session key,
domain name)&lt;/code> (so it will remain consistent if the user reloads the browser
window but is unlikely to clash with other nodes).&lt;/li>
&lt;li>There&amp;#39;s a simple &lt;a href="https://github.com/shosti/wraft/tree/main/webrtc-introducer">WebSocket-based &amp;#34;introducer&amp;#34; signaling service&lt;/a> (written in
Rust, of course) that basically just forwards &lt;a href="https://en.wikipedia.org/wiki/Session_Description_Protocol">SDP messages&lt;/a> between nodes and
announces when new nodes join the cluster.&lt;/li>
&lt;li>Nodes with smaller node IDs are responsible for introducing themselves to
nodes with bigger node IDs. (The &amp;#34;Smallest Node ID Goes First Protocol&amp;#34;
(SNIGFP?) was the simplest way I could think of to make it work.)&lt;/li>
&lt;/ul>
&lt;p>Overall it&amp;#39;s only about 250 lines of &lt;a href="https://github.com/shosti/wraft/blob/main/webrtc-introducer/src/main.rs">server-side code&lt;/a> and 500 lines of
&lt;a href="https://github.com/shosti/wraft/blob/main/wraft/src/webrtc_rpc/introduction.rs">browser-side code&lt;/a>, but it&amp;#39;s the area I&amp;#39;m least confident about (there are
definitely bugs with Safari, and I&amp;#39;m pretty sure there are some lurking race
conditions on other browsers as well).&lt;/p>
&lt;p>
To get from WebRTC data channels to something that&amp;#39;s usable for implementing
Raft, I hacked up a &lt;a href="https://github.com/shosti/wraft/blob/main/wraft/src/webrtc_rpc/transport.rs">half-baked RPC framework&lt;/a> that uses &lt;a href="https://github.com/bincode-org/bincode">bincode&lt;/a> for message
serialization (I won&amp;#39;t go into details to keep this post from getting too long,
but it was fun to implement 😄).&lt;/p>
&lt;/div>
&lt;/div>
&lt;div id="outline-container-headline-6" class="outline-3">
&lt;h3 id="headline-6">
About As Hard As Expected: Raft
&lt;/h3>
&lt;div id="outline-text-headline-6" class="outline-text-3">
&lt;p>Raft was originally billed as an &amp;#34;understandable&amp;#34; alternative to &lt;a href="https://en.wikipedia.org/wiki/Paxos_(computer_science)">Paxos&lt;/a> (which
was previously considered the &amp;#34;go-to&amp;#34; distributed consensus algorithm but is
&lt;a href="http://www.read.seas.harvard.edu/~kohler/class/08w-dsi/chandra07paxos.pdf">notoriously difficult&lt;/a> to use in practice). That may be true relatively speaking,
but after implementing Raft I would definitely not say it&amp;#39;s easy to understand!
There are a whole lot of variables to keep track of across nodes and subtle ways
things can go very wrong (I had some &lt;a href="https://github.com/shosti/wraft/commit/d369b6a9aef7c932a8ac4c2f1364c33f22498583">nasty&lt;/a> &lt;a href="https://github.com/shosti/wraft/commit/0ccdbca00bd066b812d953b6576de99e3bf0386d">bugs&lt;/a> from not properly handling
duplicate and out-of-order requests, for instance).&lt;/p>
&lt;p>
What Raft really has going for it is a fantastic &lt;a href="https://raft.github.io/raft.pdf">descriptive paper&lt;/a> and, even
better, a &lt;a href="https://github.com/ongardie/raft.tla">formal TLA⁺ specification&lt;/a> that&amp;#39;s pretty readable (even for someone
like me who doesn&amp;#39;t know TLA⁺ very well). And once the algorithm has been
implemented you end up with a usable real-world system (in contrast to Paxos,
which is more of a basic building block that leaves out most of the practical
details).&lt;/p>
&lt;p>
For WRaft, I stuck as close as possible to the TLA⁺ spec to avoid surprises. I
didn&amp;#39;t implement the trickier &amp;#34;optional&amp;#34; parts of Raft like cluster membership
changes and log compaction, even though they would probably be essential to make
WRaft useful for real-world apps. (Maybe some day if I have time…)&lt;/p>
&lt;/div>
&lt;/div>
&lt;div id="outline-container-headline-7" class="outline-3">
&lt;h3 id="headline-7">
About As Hard As Expected: Rust
&lt;/h3>
&lt;div id="outline-text-headline-7" class="outline-text-3">
&lt;p>I don&amp;#39;t really want this to turn into yet another &amp;#34;&lt;a href="https://www.google.com/search?hl=en&amp;amp;q=thoughts%20on%20rust">Thoughts on Rust&lt;/a>&amp;#34; post, but I
will nevertheless take this opportunity to rant a little about the Rust learning
curve. Specifically: &lt;em>Why are there so many confusing names?&lt;/em> Why &lt;code>Send&lt;/code> and
&lt;code>Sync&lt;/code> instead of, I dunno, &lt;code>Threadsafe&lt;/code> and &lt;code>Atomic&lt;/code>? Does anyone really think
&lt;code>Box&lt;/code>, &lt;code>Rc&lt;/code>, &lt;code>Arc&lt;/code>, etc. are the most clear names for smart pointers? (How about
&lt;code>Ptr&lt;/code> and &lt;code>SharedPtr&lt;/code> or something along those lines?) I was piling on
horrifically ugly hacks for days until I found the indispensable &lt;a href="https://doc.rust-lang.org/std/option/enum.Option.html#method.take">&lt;code>Option.take()&lt;/code>&lt;/a>
method (the docs are fine, but how are you supposed to find it in the first
place?). &lt;a href="https://docs.rs/futures/0.3.18/futures/future/trait.FusedFuture.html">&lt;code>FusedFuture&lt;/code>&lt;/a>s confounded me until I realized it was &amp;#34;fuse&amp;#34; as in &amp;#34;fuse
box&amp;#34;, not &amp;#34;Asian Fusion&amp;#34;. I lost count of how many times I misspelled
&lt;a href="https://docs.rs/futures/0.3.18/futures/channel/mpsc/index.html">&lt;code>mpsc&lt;/code>&lt;/a>. And don&amp;#39;t even get me started on the inscrutable &lt;code>Pin&lt;/code> / &lt;code>Unpin&lt;/code> stuff.&lt;/p>
&lt;p>
(Oh and while we&amp;#39;re at it: can we please have a moratorium on the use of &lt;code>static&lt;/code>
as a keyword in programming languages? It means like eleven different things
across languages and at least three in Rust.)&lt;/p>
&lt;p>
But those are fairly minor gripes when it comes down to it, and overall I enjoy
programming in Rust now that I&amp;#39;ve done my requisite time wrestling with the
compiler. (My breakthrough moment came when I realized that &amp;#34;moving&amp;#34; a value
kind of means that the value has been &amp;#34;eaten&amp;#34;; I&amp;#39;ve since started mentally
reading compiler errors as &amp;#34;use of devoured value&amp;#34; and so forth.) There&amp;#39;s
something oddly addictive about doing a big refactor and having all the types
click into place like &lt;a href="https://www.reddit.com/r/oddlysatisfying/comments/qp7sg1/the_way_these_puzzle_pieces_fit_into_place/">weirdly-shaped puzzle pieces&lt;/a>.&lt;/p>
&lt;p>
I&amp;#39;ve heard it said that the Rust language guides programmers to make &amp;#34;better&amp;#34;
design choices; I&amp;#39;m not entirely sure I buy that, but certain approaches
definitely end up causing fewer headaches than others. I thought I&amp;#39;d go through
an example for interest&amp;#39;s sake.&lt;/p>
&lt;div id="outline-container-headline-8" class="outline-4">
&lt;h4 id="headline-8">
Shared State Sucks in Rust
&lt;/h4>
&lt;div id="outline-text-headline-8" class="outline-text-4">
&lt;p>To be clear, shared state sucks in every programming language, but Rust really
makes you feel the pain up front! This is especially true in asynchronous code:
any piece of data that could ever be mutated across &lt;code>async&lt;/code> blocks has to be
wrapped in a synchronization primitive like &lt;code>Mutex&lt;/code> or &lt;code>RwLock&lt;/code>, which makes the
code get ugly really quick. (It seems a bit pedantic that this is all enforced
in Wasm modules, where everything is single-threaded, but I understand the
rationale.)&lt;/p>
&lt;p>
This came up in a big way when implementing Raft. Raft nodes basically have
three jobs:&lt;/p>
&lt;ul>
&lt;li>Respond to client requests (e.g. to read or write data)&lt;/li>
&lt;li>Respond to RPC requests from other nodes&lt;/li>
&lt;li>Do various &amp;#34;active&amp;#34; tasks like calling elections and sending heartbeats other
nodes&lt;/li>
&lt;/ul>
&lt;p>The &amp;#34;obvious&amp;#34; way to implement this is to have several tasks running
concurrently with some shared state. That was my initial approach, but the code
was soon drowning in a sea of &lt;code>Arc&amp;lt;Mutex&amp;lt;T&amp;gt;&amp;gt;&lt;/code> s and I was even running into
deadlock issues. I thought there had to be a better way.&lt;/p>
&lt;p>
What I ended up with was (ironically) a very Go-like approach oriented around
&lt;a href="https://docs.rs/futures/0.3.18/futures/channel/index.html">channels&lt;/a>. Lots and lots of channels (about twenty overall in the project!). The
basic idea is that there&amp;#39;s always a single event loop that does all the heavy
lifting, with client/RPC requests coming in through channels (mostly &lt;a href="https://docs.rs/futures/0.3.18/futures/channel/mpsc/index.html">mpsc&lt;/a>
channels from the &lt;a href="https://docs.rs/futures/0.3.18/futures/index.html">futures crate&lt;/a>, which behave fairly similarly to &lt;a href="https://go.dev/tour/concurrency/2">Go channels&lt;/a>).&lt;/p>
&lt;p>
For a concrete example, here&amp;#39;s what the event loop looks like (more or less) for
a Raft worker that&amp;#39;s in the &lt;code>Follower&lt;/code> state:&lt;/p>
&lt;div class="src src-rust">
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-rust" data-lang="rust">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#fff;font-weight:bold">loop&lt;/span> {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#007f7f">// ...
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#007f7f">&lt;/span> select! {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#007f7f">// Handle RPC requests
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#007f7f">&lt;/span> res = self.inner.rpc_rx.next() =&amp;gt; {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#fff;font-weight:bold">let&lt;/span> (req, resp_tx) = res.expect(&lt;span style="color:#0ff;font-weight:bold">&amp;#34;RPC channel closed&amp;#34;&lt;/span>);
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> self.handle_rpc(req, resp_tx, &amp;amp;&lt;span style="color:#fff;font-weight:bold">mut&lt;/span> timeout);
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> }
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#007f7f">// Handle client requests (they get forwarded to the leader)
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#007f7f">&lt;/span> res = self.inner.client_rx.next() =&amp;gt; {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#fff;font-weight:bold">let&lt;/span> (req, resp_tx) = res.expect(&lt;span style="color:#0ff;font-weight:bold">&amp;#34;Client channel closed&amp;#34;&lt;/span>);
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#fff;font-weight:bold">match&lt;/span> req {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#007f7f">// ...
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#007f7f">&lt;/span> ClientRequest::Apply(_) =&amp;gt; self.forward_client_request(req, resp_tx),
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> }
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> }
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#007f7f">// Timeout for when there aren&amp;#39;t heartbeats from the leader
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#007f7f">&lt;/span> _ = timeout =&amp;gt; {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> console_log!(&lt;span style="color:#0ff;font-weight:bold">&amp;#34;Calling election!&amp;#34;&lt;/span>);
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#fff;font-weight:bold">return&lt;/span> RaftWorkerState::Candidate(self.into());
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> }
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> }
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>}&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>
&lt;/div>
&lt;p>
Basically, a Raft follower sits there waiting for requests either from peers
(the &lt;code>rpc_rx&lt;/code> channel) or clients (the &lt;code>client_rx&lt;/code> channel). If it doesn&amp;#39;t
receive anything from the leader for a while, it assumes the leader is down and
it calls an election. (There are similar event loops for Raft workers in the
&lt;code>Leader&lt;/code> and &lt;code>Candidate&lt;/code> states.)&lt;/p>
&lt;p>
The tricky part is handling request/response patterns (which is how all Raft
RPCs are designed), since channels are inherently unidirectional. The solution
is to bundle each request with a &lt;a href="https://docs.rs/futures/0.3.18/futures/channel/oneshot/index.html">oneshot&lt;/a> reply channel, which is basically a
&amp;#34;single-use&amp;#34; channel that will only ever get one value. (The Rust compiler will
even ensure that oneshot channels can only be used once, which is pretty cool.)&lt;/p>
&lt;p>
To make this all a bit more concrete, here&amp;#39;s the code that handles RPC requests:&lt;/p>
&lt;div class="src src-rust">
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-rust" data-lang="rust">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#fff;font-weight:bold">async&lt;/span> &lt;span style="color:#fff;font-weight:bold">fn&lt;/span> handle(&amp;amp;self, req: RpcRequest&amp;lt;Cmd&amp;gt;) -&amp;gt; &lt;span style="color:#fff;font-weight:bold">Result&lt;/span>&amp;lt;RpcResponse&amp;lt;Cmd&amp;gt;, transport::Error&amp;gt; {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#007f7f">// We make a oneshot response channel for each request
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#007f7f">&lt;/span> &lt;span style="color:#fff;font-weight:bold">let&lt;/span> (resp_tx, resp_rx) = oneshot::channel();
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#007f7f">// self.tx is the &amp;#34;sender&amp;#34; half of the RPC channel; the &amp;#34;receiver&amp;#34; half is
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#007f7f">&lt;/span> &lt;span style="color:#007f7f">// in the Raft worker event loop
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#007f7f">&lt;/span> self.tx
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> .clone()
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> .send((req, resp_tx)) &lt;span style="color:#007f7f">// send both the request and the response channel as a tuple
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#007f7f">&lt;/span> .&lt;span style="color:#fff;font-weight:bold">await&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> .expect(&lt;span style="color:#0ff;font-weight:bold">&amp;#34;request channel closed&amp;#34;&lt;/span>);
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#007f7f">// Now we wait for the response, which the Raft worker should (hopefully)
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#007f7f">&lt;/span> &lt;span style="color:#007f7f">// send us
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#007f7f">&lt;/span> &lt;span style="color:#fff;font-weight:bold">match&lt;/span> resp_rx.&lt;span style="color:#fff;font-weight:bold">await&lt;/span> {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#fff;font-weight:bold">Ok&lt;/span>(resp) =&amp;gt; resp,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#007f7f">// ...
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#007f7f">&lt;/span> }
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>}&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>
&lt;/div>
&lt;p>
Thanks to the use of channels, the RPC handler is completely independent from
the Raft worker; there&amp;#39;s no shared state, which means the Raft worker task can
&amp;#34;own&amp;#34; all its mutable state, which means no mutexes, which means much nicer code
overall!&lt;/p>
&lt;p>
The big drawback I&amp;#39;ve found to this approach is error handling. You can send
&lt;code>Result&lt;/code> types over channels (which is what I often ended up doing), but you
lose the goodness of the &lt;code>?&lt;/code> operator and you&amp;#39;re left with the very sticky issue
of how to handle errors on the channels themselves. My channel-related code
ended up with a lot of &lt;code>unwrap()&lt;/code> and &lt;code>expect()&lt;/code>, which is far from ideal (and
caused a fair number of actual bugs). (This project has given me a new
appreciation for &lt;a href="https://www.erlang.org/doc/man/supervisor.html">Erlang supervisors&lt;/a>, which are are an interesting approach to
dealing with these sorts of problems.)&lt;/p>
&lt;p>
Is this the &amp;#34;best&amp;#34; way to handle state in asynchronous Rust code? I have no
idea! I haven&amp;#39;t dug into the performance implications of using channels
vs. mutexes, and experienced Rustaceans probably know some tricks to make it all
a lot more elegant. (I experimented a bit with combinators like
&lt;a href="https://docs.rs/futures/0.3.18/futures/stream/struct.FuturesUnordered.html">&lt;code>FuturesUnordered&lt;/code>&lt;/a>, but without much success.) But using channels definitely
made it easier to wrap my head around the program control flow. Also, the code
ended up looking a lot more like the Raft TLA⁺ spec which made the
implementation easier 😄.&lt;/p>
&lt;/div>
&lt;/div>
&lt;/div>
&lt;/div>
&lt;/div>
&lt;/div>
&lt;div id="outline-container-headline-9" class="outline-2">
&lt;h2 id="headline-9">
Closing Thoughts
&lt;/h2>
&lt;div id="outline-text-headline-9" class="outline-text-2">
&lt;p>Overall, I think Rust/Wasm is a pretty compelling platform for high-performance
libraries for frontend applications. (I&amp;#39;m less convinced that it&amp;#39;s a good choice
for React-style SPAs, but it&amp;#39;s surprisingly usable even for that.) The biggest
limitation I&amp;#39;ve found is the lack of true multi-threading. Single-threaded
asynchronous code is easy thanks to the &lt;a href="https://docs.rs/wasm-bindgen-futures/0.4.28/wasm_bindgen_futures/">wasm_bindgen_futures&lt;/a> crate, but &amp;#34;real&amp;#34;
threads with shared memory still seem to be out of reach. There&amp;#39;s been some
&lt;a href="https://rustwasm.github.io/2018/10/24/multithreading-rust-and-wasm.html">interesting activity&lt;/a> around multi-threading but sadly it looks like the
development effort has stalled.&lt;/p>
&lt;p>
One project I&amp;#39;m definitely going to keep an eye on is &lt;a href="https://wasi.dev/">WASI&lt;/a>, which is an attempt
to implement standardized &amp;#34;system calls&amp;#34; for Wasm (inside and outside of the
browser). I&amp;#39;m still not entirely clear how that would look in the context of
frontend applications—a lot of extremely useful system calls (like the ones that
deal with files and TCP sockets) don&amp;#39;t really have analogs in the browser—but
I&amp;#39;m definitely excited by the prospect of some day writing frontend apps without
having to deal with crusty JavaScript-based Web APIs!&lt;/p>
&lt;/div>
&lt;/div>
&lt;div id="outline-container-headline-10" class="outline-2">
&lt;h2 id="headline-10">
What Next?
&lt;/h2>
&lt;div id="outline-text-headline-10" class="outline-text-2">
&lt;p>Maybe nothing? WRaft wasn&amp;#39;t designed with any real-world use case in mind; I was
mostly just curious to see if it was possible. But I could see it potentially
being useful for a peer-to-peer group chat app, multiplayer browser-based game,
or something along those lines. (An interesting side-effect of using WebRTC is
that all communication is end-to-end encrypted, so WRaft ended up surprisingly
secure basically by accident…)&lt;/p>
&lt;p>
If you can think of a use case, &lt;a href="https://github.com/shosti/wraft/issues">open an issue&lt;/a> and maybe I&amp;#39;ll end turning it into
a proper crate or NPM package.&lt;/p>
&lt;/div>
&lt;/div></description></item><item><title>The Simplest Multi-Node Kubernetes Cluster</title><link>https://eevans.co/blog/kubernetes-multi-node/</link><pubDate>Mon, 10 Aug 2020 00:00:00 +0000</pubDate><guid>https://eevans.co/blog/kubernetes-multi-node/</guid><category>kubernetes</category><category>k8s</category><category>cni</category><category>openstack</category><category>technical</category><description>
&lt;p>
In &lt;a href="https://eevans.co/blog/deconstructing-kubernetes-networking">the last post of this series&lt;/a>, we got networking working on a single
Kubernetes node using the &lt;a href="https://github.com/containernetworking/plugins/tree/master/plugins/main/bridge">bridge CNI plugin&lt;/a>. (If you haven&amp;#39;t read that post, you
might want to take a quick look—I&amp;#39;m going to assume you&amp;#39;ve read and maybe even
understood it 😉.) So let&amp;#39;s get our cluster set up with multiple nodes!
Thankfully it won&amp;#39;t be too difficult now that we have a good idea of how
container networking and CNI work.&lt;/p>
&lt;div id="outline-container-headline-1" class="outline-2">
&lt;h2 id="headline-1">
Prerequisites for Following Along
&lt;/h2>
&lt;div id="outline-text-headline-1" class="outline-text-2">
&lt;p>The bad news about Kubernetes networking is that it&amp;#39;s hard to set up in a truly
generic way. Each cloud provider handles networking slightly differently, and
cloud networking is quite a bit different from what you&amp;#39;d typically see in a
&amp;#34;traditional&amp;#34; data center. I set this up in my homelab, which runs OpenStack (a
mildly insane choice—I&amp;#39;ll probably give more details in a future blog
post). Things will probably look a bit different if you&amp;#39;re trying to follow
along in AWS or GCP.&lt;/p>
&lt;p>
With that caveat, here&amp;#39;s the setup I used:&lt;/p>
&lt;ul>
&lt;li>One VM named &lt;code>mink8s&lt;/code> (the one we&amp;#39;ve been working with so far), with internal
IP &lt;code>10.70.10.228&lt;/code>.&lt;/li>
&lt;li>Another VM named &lt;code>mink8s2&lt;/code>, with internal IP &lt;code>10.70.10.248&lt;/code>.&lt;/li>
&lt;/ul>
&lt;p>Both VMs are attached to the same network (this would be a VPC in AWS or GCP),
with port security disabled.&lt;/p>
&lt;p>
Yes that&amp;#39;s right—I completely disabled network-level security and firewalls to
get this working (more on that later). In the spirit of this whole blog series
so far, the setup is &lt;strong>spectacularly insecure&lt;/strong>. Not only should you not use it in
production, but please don&amp;#39;t try this on anything with a public IP. A good
option is to set this up locally with something like VirtualBox, but it should
also work with your cloud provider of choice with some tweaking (I haven&amp;#39;t
tested it yet on anything other than OpenStack).&lt;/p>
&lt;p>
In keeping with the &lt;a href="https://kubernetes.io/docs/concepts/cluster-administration/networking/#the-kubernetes-network-model">Kubernetes network model&lt;/a> that we discussed last time, you&amp;#39;ll
need to make sure that your two VMs can talk to each other directly over all TCP
and UDP ports.&lt;/p>
&lt;/div>
&lt;/div>
&lt;div id="outline-container-headline-2" class="outline-2">
&lt;h2 id="headline-2">
The High-Level Approach
&lt;/h2>
&lt;div id="outline-text-headline-2" class="outline-text-2">
&lt;p>There are two basic approaches to multi-node Kubernetes networking:&lt;/p>
&lt;ol>
&lt;li>Use an &lt;strong>overlay network&lt;/strong>, which is a virtual network that sits on top of your
&amp;#34;real&amp;#34; (aka &amp;#34;underlay&amp;#34;) network. In this setup, packets that move between
hosts are encapsulated somehow (&lt;a href="https://en.wikipedia.org/wiki/Virtual_Extensible_LAN">VXLAN&lt;/a> seems to be a popular choice), and the
network that your pods &amp;#34;see&amp;#34; will be different from your host&amp;#39;s network. This
is the approach that &lt;a href="https://github.com/coreos/flannel">flannel&lt;/a> uses by default.&lt;/li>
&lt;li>Use the &lt;strong>native network&lt;/strong> (i.e. whatever network your Kubernetes hosts are
attached to) and route traffic using standard IP routing protocols. This
means that additional work has to be done to set up routes and
non-conflicting IP addresses (one popular way to set up routes is to use
&lt;a href="https://en.wikipedia.org/wiki/Border_Gateway_Protocol">BGP&lt;/a>). This is the approach that &lt;a href="https://www.projectcalico.org/">Calico&lt;/a> uses by default.&lt;/li>
&lt;/ol>
&lt;p>Both approaches have advantages and disadvantages, and in the real world the
split isn&amp;#39;t always clean—for instance, if you&amp;#39;re running in a cloud environment
you&amp;#39;re probably already using an overlay network for your VMs. But we&amp;#39;re going
with approach 2 for the boring reason that it will be easier to set up. We&amp;#39;ll
sidestep the tricky issues of IP overlaps and routing by doing some good
old-fashioned hardcoding and manual setup.&lt;/p>
&lt;/div>
&lt;/div>
&lt;div id="outline-container-headline-3" class="outline-2">
&lt;h2 id="headline-3">
Getting the Second Node Up and Running
&lt;/h2>
&lt;div id="outline-text-headline-3" class="outline-text-2">
&lt;p>If you followed along with the last post, our second Kubernetes node should be
almost ready. For the lazy reader, here are the instructions for getting it up
and running:&lt;/p>
&lt;div class="src src-text">
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-text" data-lang="text">&lt;span style="display:flex;">&lt;span>ubuntu@mink8s2:~$ sudo apt install docker.io
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>ubuntu@mink8s2:~$ sudo systemctl enable docker
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>ubuntu@mink8s2:~$ sudo systemctl start docker
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>ubuntu@mink8s2:~$ curl -L https://storage.googleapis.com/kubernetes-release/release/v1.18.5/bin/linux/amd64/kubelet &amp;gt; kubelet
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>ubuntu@mink8s2:~$ curl -L https://storage.googleapis.com/kubernetes-release/release/v1.18.5/bin/linux/amd64/kubectl &amp;gt; kubectl
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>ubuntu@mink8s2:~$ chmod +x kubelet
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>ubuntu@mink8s2:~$ chmod +x kubectl
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>ubuntu@mink8s2:~$ API_IP=10.70.10.228 # set to your original node&amp;#39;s IP
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>ubuntu@mink8s2:~$ cat &amp;lt;&amp;lt;EOS &amp;gt; kubeconfig.yaml
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>apiVersion: v1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>kind: Config
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>clusters:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>- cluster:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> server: http://$API_IP:8080
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> name: mink8s
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>contexts:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>- context:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> cluster: mink8s
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> name: mink8s
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>current-context: mink8s
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>EOS
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>ubuntu@mink8s2:~$ mkdir -p .kube
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>ubuntu@mink8s2:~$ cp kubeconfig.yaml .kube/config&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>
&lt;/div>
&lt;p>
We&amp;#39;ll also have to install the official CNI plugins and adjust Docker settings,
just like we did on the first node:&lt;/p>
&lt;div class="src src-text">
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-text" data-lang="text">&lt;span style="display:flex;">&lt;span>$ curl -L https://github.com/containernetworking/plugins/releases/download/v0.8.6/cni-plugins-linux-amd64-v0.8.6.tgz &amp;gt; cni.tgz
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>$ sudo mkdir -p /opt/cni/bin
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>$ sudo tar xzvf cni.tgz -C /opt/cni/bin
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>$ cat &amp;lt;&amp;lt;EOS | sudo tee /etc/docker/daemon.json
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>{
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &amp;#34;exec-opts&amp;#34;: [&amp;#34;native.cgroupdriver=cgroupfs&amp;#34;],
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &amp;#34;log-driver&amp;#34;: &amp;#34;json-file&amp;#34;,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &amp;#34;log-opts&amp;#34;: {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &amp;#34;max-size&amp;#34;: &amp;#34;100m&amp;#34;
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> },
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &amp;#34;bridge&amp;#34;: &amp;#34;none&amp;#34;,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &amp;#34;iptables&amp;#34;: false,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &amp;#34;ip-masq&amp;#34;: false,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &amp;#34;storage-driver&amp;#34;: &amp;#34;overlay2&amp;#34;
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>}
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>EOS&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>
&lt;/div>
&lt;p>
(And I can&amp;#39;t stress enough: after you change that Docker config file, &lt;em>reboot&lt;/em>!
I don&amp;#39;t want to disclose how many hours of hair-pulling I went through debugging
CNI&amp;lt;&amp;gt;Docker networking issues.)&lt;/p>
&lt;p>
Finally, we&amp;#39;ll make our CNI configuration in &lt;code>/etc/cni/net.d/bridge.conf&lt;/code>:&lt;/p>
&lt;div class="src src-javascript">
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-javascript" data-lang="javascript">&lt;span style="display:flex;">&lt;span>{
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#0ff;font-weight:bold">&amp;#34;cniVersion&amp;#34;&lt;/span>: &lt;span style="color:#0ff;font-weight:bold">&amp;#34;0.3.1&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#0ff;font-weight:bold">&amp;#34;name&amp;#34;&lt;/span>: &lt;span style="color:#0ff;font-weight:bold">&amp;#34;mink8s&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#0ff;font-weight:bold">&amp;#34;type&amp;#34;&lt;/span>: &lt;span style="color:#0ff;font-weight:bold">&amp;#34;bridge&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#0ff;font-weight:bold">&amp;#34;bridge&amp;#34;&lt;/span>: &lt;span style="color:#0ff;font-weight:bold">&amp;#34;mink8s0&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#0ff;font-weight:bold">&amp;#34;isGateway&amp;#34;&lt;/span>: &lt;span style="color:#fff;font-weight:bold">true&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#0ff;font-weight:bold">&amp;#34;ipMasq&amp;#34;&lt;/span>: &lt;span style="color:#fff;font-weight:bold">true&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#0ff;font-weight:bold">&amp;#34;mtu&amp;#34;&lt;/span>: &lt;span style="color:#ff0;font-weight:bold">1450&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#0ff;font-weight:bold">&amp;#34;ipam&amp;#34;&lt;/span>: {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#0ff;font-weight:bold">&amp;#34;type&amp;#34;&lt;/span>: &lt;span style="color:#0ff;font-weight:bold">&amp;#34;host-local&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#0ff;font-weight:bold">&amp;#34;ranges&amp;#34;&lt;/span>: [
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> [{&lt;span style="color:#0ff;font-weight:bold">&amp;#34;subnet&amp;#34;&lt;/span>: &lt;span style="color:#0ff;font-weight:bold">&amp;#34;10.12.2.0/24&amp;#34;&lt;/span>}]
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> ],
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#0ff;font-weight:bold">&amp;#34;routes&amp;#34;&lt;/span>: [{&lt;span style="color:#0ff;font-weight:bold">&amp;#34;dst&amp;#34;&lt;/span>: &lt;span style="color:#0ff;font-weight:bold">&amp;#34;0.0.0.0/0&amp;#34;&lt;/span>}]
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> }
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>}&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>
&lt;/div>
&lt;p>
This looks almost identical to the config on our original node with one
important difference: we&amp;#39;re using the &lt;code>10.12.2.0/24&lt;/code> subnet for our pods instead
of &lt;code>10.12.1.0/24&lt;/code>. Since we&amp;#39;re using &lt;code>host-local&lt;/code> IPAM, we&amp;#39;ll have to manually
make sure that each node gets a non-overlapping set of IPs to use for pods. (It
doesn&amp;#39;t really matter what the ranges are as long as they don&amp;#39;t overlap with
anything else on the network.)&lt;/p>
&lt;p>
The sharp-eyed reader will also notice the new &lt;code>mtu&lt;/code> option. That just sets the
&lt;a href="https://en.wikipedia.org/wiki/Maximum_transmission_unit">MTU&lt;/a> on our pods&amp;#39; and bridge&amp;#39;s virtual network devices. I&amp;#39;m including it here
because OpenStack virtual network devices have an MTU of 1450 (instead of the
standard value of 1500). You should just set it to whatever your host&amp;#39;s Ethernet
adapter&amp;#39;s value is (you can find it pretty easily with &lt;code>ip link&lt;/code> —it&amp;#39;s different
across different cloud providers). If your pods&amp;#39; MTU is greater than your host&amp;#39;s
link&amp;#39;s MTU, you might run into &lt;a href="https://en.wikipedia.org/wiki/IP_fragmentation">IP fragmentation&lt;/a> issues when communicating across
nodes, which are a nightmare to debug. (You should set this option on your
original &lt;code>mink8s&lt;/code> node as well.)&lt;/p>
&lt;p>
Anyways, let&amp;#39;s fire up kubelet to get our node up and running:&lt;/p>
&lt;div class="src src-text">
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-text" data-lang="text">&lt;span style="display:flex;">&lt;span>sudo ./kubelet --network-plugin=cni --kubeconfig=kubeconfig.yaml&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>
&lt;/div>
&lt;p>
And we&amp;#39;ll also run a &lt;code>sleep&lt;/code> pod on the new node:&lt;/p>
&lt;div class="src src-text">
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-text" data-lang="text">&lt;span style="display:flex;">&lt;span>$ cat &amp;lt;&amp;lt;EOS | ./kubectl apply -f -
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>apiVersion: v1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>kind: Pod
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>metadata:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> name: sleep3
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>spec:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> containers:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - image: alpine
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> name: alpine
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> command: [&amp;#34;sleep&amp;#34;, &amp;#34;5000000&amp;#34;]
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> nodeName: mink8s2
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>EOS&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>
&lt;/div>
&lt;p>
It looks like things are going OK so far; our node has been registered and our
pod gets an IP in the right range at least:&lt;/p>
&lt;div class="src src-text">
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-text" data-lang="text">&lt;span style="display:flex;">&lt;span>$ ./kubectl get no
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>NAME STATUS ROLES AGE VERSION
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>mink8s Ready &amp;lt;none&amp;gt; 33d v1.18.5
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>mink8s2 Ready &amp;lt;none&amp;gt; 5d21h v1.18.5
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>$ ./kubectl get po -owide
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>sleep1 1/1 Running 0 43m 10.12.1.15 mink8s &amp;lt;none&amp;gt; &amp;lt;none&amp;gt;
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>Sleep2 1/1 Running 0 104m 10.12.1.14 mink8s &amp;lt;none&amp;gt; &amp;lt;none&amp;gt;
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>sleep3 1/1 Running 0 25m 10.12.2.6 mink8s2 &amp;lt;none&amp;gt; &amp;lt;none&amp;gt;&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>
&lt;/div>
&lt;p>
But pods still can&amp;#39;t ping each other across nodes, so we have some work to do:&lt;/p>
&lt;div class="src src-text">
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-text" data-lang="text">&lt;span style="display:flex;">&lt;span>$ ./kubectl exec sleep1 -- ping -c 3 10.12.2.2
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>PING 10.12.2.2 (10.12.2.2): 56 data bytes
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>--- 10.12.2.2 ping statistics ---
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>3 packets transmitted, 0 packets received, 100% packet loss
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>command terminated with exit code 1&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>
&lt;/div>
&lt;/div>
&lt;/div>
&lt;div id="outline-container-headline-4" class="outline-2">
&lt;h2 id="headline-4">
A Packet&amp;#39;s Incredible Journey
&lt;/h2>
&lt;div id="outline-text-headline-4" class="outline-text-2">
&lt;p>At this point it&amp;#39;s worth zooming out a bit to understand how we&amp;#39;re expecting our
network packets to get to pods in other nodes. Here&amp;#39;s a rough diagram of what
we&amp;#39;re looking for (with IPs from my setup):&lt;/p>
&lt;p>
&lt;figure>
&lt;img src="https://eevans.co/blog/kubernetes-multi-node/multi-node-network.svg" />
&lt;/figure>
&lt;/p>
&lt;p>
So any packet that goes across nodes will have to deal with three hops:&lt;/p>
&lt;ol>
&lt;li>First, it will have to get from the pod to the relevant bridge (via the veth
pair we discussed in the last post).&lt;/li>
&lt;li>Next, it will have to get to the destination node over the &amp;#34;real&amp;#34; network
adapter, &lt;code>ens3&lt;/code> in my case. (Implicitly, the packet has to get from the
bridge to the network adapter, but that&amp;#39;s handled internally by the kernel.)&lt;/li>
&lt;li>Once the packet arrives at its destination host, it has to be routed through
the relevant bridge to the destination pod.&lt;/li>
&lt;/ol>
&lt;p>It turns out that we have very little work to do to get this routing setup
working. In fact, hops 1 and 3 have already been set up by the CNI &lt;code>bridge&lt;/code>
plugin.&lt;/p>
&lt;p>
First let&amp;#39;s check hop 1 by examining the pod&amp;#39;s routing table:&lt;/p>
&lt;div class="src src-text">
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-text" data-lang="text">&lt;span style="display:flex;">&lt;span>$ ./kubectl exec sleep1 -- ip route
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>default via 10.12.1.1 dev eth0
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>10.12.1.0/24 dev eth0 scope link src 10.12.1.15&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>
&lt;/div>
&lt;p>
The default route goes to &lt;code>10.12.1.1&lt;/code>, which is the bridge&amp;#39;s IP address—exactly
what we want. (If you remember, this was specified in our CNI network config
under &lt;code>ipam.routes&lt;/code>.)&lt;/p>
&lt;p>
For hop 3, we&amp;#39;ll check the routing table on the &lt;code>mink8s2&lt;/code> node:&lt;/p>
&lt;div class="src src-text">
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-text" data-lang="text">&lt;span style="display:flex;">&lt;span>ubuntu@mink8s2:~$ ip route
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>default via 10.70.0.1 dev ens3 proto dhcp src 10.70.10.248 metric 100
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>10.12.2.0/24 dev mink8s0 proto kernel scope link src 10.12.2.1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>10.70.0.0/16 dev ens3 proto kernel scope link src 10.70.10.248
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>169.254.169.254 via 10.70.10.1 dev ens3 proto dhcp src 10.70.10.248 metric 100&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>
&lt;/div>
&lt;p>
Packets destined for &lt;code>10.12.2.0/24&lt;/code> will be routed through our &lt;code>mink8s0&lt;/code> bridge,
which again is exactly what we want. This route was automatically set up for us
by the &lt;code>bridge&lt;/code> plugin.&lt;/p>
&lt;p>
So hop 2 is the only one we have to worry about. Since &lt;code>10.12.2.0/24&lt;/code> isn&amp;#39;t part
of our routing table, the kernel will try to route its packets over the default
route, which happens to be &lt;code>10.70.0.1&lt;/code> (the Internet gateway). That obviously
won&amp;#39;t work (unless the gateway itself has some fancy routing configuration—hold
onto that thought), but we can just add a route manually using the &lt;code>ip&lt;/code> command:&lt;/p>
&lt;div class="src src-text">
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-text" data-lang="text">&lt;span style="display:flex;">&lt;span>ubuntu@mink8s:~$ sudo ip route add 10.12.2.0/24 via 10.70.10.248 dev ens3&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>
&lt;/div>
&lt;p>
Which translates to &amp;#34;route any packet destined for &lt;code>10.12.2.0/24&lt;/code> through
&lt;code>10.70.10.248&lt;/code> (our &lt;code>mink8s2&lt;/code> node) over the &lt;code>ens3&lt;/code> link. Of course we&amp;#39;ll also
want response packets to be able to get to our &lt;code>mink8s&lt;/code> node, so we have to make
a corresponding route on the &lt;code>mink8s2&lt;/code> node:&lt;/p>
&lt;div class="src src-text">
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-text" data-lang="text">&lt;span style="display:flex;">&lt;span>ubuntu@mink8s2:~$ sudo ip route add 10.12.1.0/24 via 10.70.10.228 dev ens3&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>
&lt;/div>
&lt;p>
Once that&amp;#39;s done, it looks like the routing works just like we expected (which
we can verify with &lt;code>traceroute&lt;/code>):&lt;/p>
&lt;div class="src src-text">
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-text" data-lang="text">&lt;span style="display:flex;">&lt;span>ubuntu@mink8s:~$ ./kubectl exec sleep1 -- ping -c 3 10.12.2.6
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>PING 10.12.2.6 (10.12.2.6): 56 data bytes
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>64 bytes from 10.12.2.6: seq=0 ttl=62 time=0.639 ms
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>64 bytes from 10.12.2.6: seq=1 ttl=62 time=0.586 ms
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>64 bytes from 10.12.2.6: seq=2 ttl=62 time=0.506 ms
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>ubuntu@mink8s:~$ ./kubectl exec sleep1 -- traceroute 10.12.2.6
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>traceroute to 10.12.2.6 (10.12.2.6), 30 hops max, 46 byte packets
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> 1 10.12.1.1 (10.12.1.1) 0.016 ms 0.065 ms 0.013 ms
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> 2 10.70.10.248 (10.70.10.248) 0.768 ms 0.491 ms 0.264 ms
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> 3 10.12.2.6 (10.12.2.6) 0.434 ms 1.041 ms 0.510 ms&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>
&lt;/div>
&lt;/div>
&lt;/div>
&lt;div id="outline-container-headline-5" class="outline-2">
&lt;h2 id="headline-5">
Success! (?)
&lt;/h2>
&lt;div id="outline-text-headline-5" class="outline-text-2">
&lt;p>So we&amp;#39;ve successfully implemented the Kubernetes model for two nodes, and all it
took was a couple &lt;code>ip route&lt;/code> commands.&lt;sup class="footnote-reference">&lt;a id="footnote-reference-1" href="#footnote-1">1&lt;/a>&lt;/sup> You can try playing around with
nginx pods on both nodes to confirm that everything looks sane.&lt;/p>
&lt;p>
To be more specific: &amp;#34;it works on my machine(s)&amp;#34; 😛. There are many reasons this
might not work for you, since as I mentioned earlier networking setups tend to
vary wildly between environments. Cloud environments usually prefer handling
routing tables at the network level instead of within VMs (in our diagram above,
that would mean that the blue switch in the middle would be where the routes are
configured).&lt;/p>
&lt;p>
Even in a more &amp;#34;traditional&amp;#34; networking setup, anti-MAC-spoofing protections and
other security measures can get in the way of this sort of routing (that&amp;#39;s why I
had to disable port security in OpenStack). If you try this out and run into
issues, drop me a line!&lt;/p>
&lt;p>
But a more serious problem is that this setup is not very scalable. We had to
execute one &lt;code>ip route&lt;/code> command for each of our two nodes, but if we have &lt;code>n&lt;/code>
total nodes we&amp;#39;ll have to add &lt;code>n-1&lt;/code> routes per node. That will get tedious very
quickly! In the next post, I&amp;#39;ll try to show how we can automate the tedium away.&lt;/p>
&lt;/div>
&lt;/div>
&lt;div class="footnotes">
&lt;hr class="footnotes-separatator">
&lt;div class="footnote-definitions">
&lt;div class="footnote-definition">
&lt;sup id="footnote-1">&lt;a href="#footnote-reference-1">1&lt;/a>&lt;/sup>
&lt;div class="footnote-body">
&lt;p>It&amp;#39;s a little unclear to me whether we&amp;#39;re conforming to the &amp;#34;pods on a
node can communicate with all pods on all nodes without NAT&amp;#34; requirement of the
networking model, since IP masquerading will apply to pod-to-pod traffic. But
this whole setup is very similar to the one in &lt;a href="https://github.com/kelseyhightower/kubernetes-the-hard-way">Kubernetes The Hard Way&lt;/a> and I
trust Kelsey Hightower to have gotten it right. See &lt;a href="https://itnext.io/kubernetes-networking-behind-the-scenes-39a1ab1792bb">this blog&lt;/a> for some more
discussion.&lt;/p>
&lt;/div>
&lt;/div>
&lt;/div>
&lt;/div></description></item><item><title>Deconstructing Kubernetes Networking</title><link>https://eevans.co/blog/deconstructing-kubernetes-networking/</link><pubDate>Tue, 04 Aug 2020 00:00:00 +0000</pubDate><guid>https://eevans.co/blog/deconstructing-kubernetes-networking/</guid><category>kubernetes</category><category>k8s</category><category>cni</category><category>openstack</category><category>technical</category><description>
&lt;p>
In &lt;a href="https://eevans.co/blog/minimum-viable-kubernetes">the first post&lt;/a> of the series I&amp;#39;ve decided to call &amp;#34;Deconstructing
Kubernetes&amp;#34;, we set up an extremely basic more-or-less-functional Kubernetes
cluster with one node. The logical next step is to go multi-node—how hard could
it be?&lt;/p>
&lt;p>
Quite hard, it turns out! (So hard that we won&amp;#39;t be able to do it in one blog
post.) As soon as we move beyond one node, we have to deal with container
networking across hosts, which involves a lot of intricacies. But it&amp;#39;s an
interesting exercise to dive into the mud and figure out how the various
networking pieces fit together.&lt;/p>
&lt;div id="outline-container-headline-1" class="outline-2">
&lt;h2 id="headline-1">
Failing to Go Multi-Node
&lt;/h2>
&lt;div id="outline-text-headline-1" class="outline-text-2">
&lt;p>Let&amp;#39;s first try to set up a naïve two-node cluster to see what we&amp;#39;re up
against. We&amp;#39;ll need to allow kubelet on the second node to talk to the API
server on our existing node. In the spirit of completely ignoring security to
keep blog post length manageable, we can just allow open access to the
Kubernetes API. Edit &lt;code>pods/kube-apiserver.yaml&lt;/code> to look like this:&lt;/p>
&lt;div class="src src-yaml">
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>&lt;span style="font-weight:bold">apiVersion&lt;/span>: v1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="font-weight:bold">kind&lt;/span>: Pod
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="font-weight:bold">metadata&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="font-weight:bold">name&lt;/span>: kube-apiserver
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="font-weight:bold">namespace&lt;/span>: kube-system
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="font-weight:bold">spec&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="font-weight:bold">containers&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - &lt;span style="font-weight:bold">name&lt;/span>: kube-apiserver
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="font-weight:bold">command&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - kube-apiserver
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - --etcd-servers=http://127.0.0.1:2379
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - --insecure-bind-address=0.0.0.0
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="font-weight:bold">image&lt;/span>: k8s.gcr.io/kube-apiserver:v1.18.5
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="font-weight:bold">hostNetwork&lt;/span>: &lt;span style="color:#fff;font-weight:bold">true&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>
&lt;/div>
&lt;p>
(Notice the &lt;code>insecure-bind-address&lt;/code> option. This setup is &lt;strong>fantastically
insecure&lt;/strong>, and you probably shouldn&amp;#39;t follow along with this section if your VMs
have public IP addresses.)&lt;/p>
&lt;p>
Next, we&amp;#39;ll set up a second VM that can communicate with the first node. The
setup is very similar to what we did for the original node; let&amp;#39;s just rush
through it:&lt;/p>
&lt;div class="src src-text">
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-text" data-lang="text">&lt;span style="display:flex;">&lt;span>ubuntu@mink8s2:~$ sudo apt install docker.io
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>ubuntu@mink8s2:~$ sudo systemctl enable docker
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>ubuntu@mink8s2:~$ sudo systemctl start docker
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>ubuntu@mink8s2:~$ curl -L https://storage.googleapis.com/kubernetes-release/release/v1.18.5/bin/linux/amd64/kubelet &amp;gt; kubelet
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>ubuntu@mink8s2:~$ curl -L https://storage.googleapis.com/kubernetes-release/release/v1.18.5/bin/linux/amd64/kubectl &amp;gt; kubectl
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>ubuntu@mink8s2:~$ chmod +x kubelet
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>ubuntu@mink8s2:~$ chmod +x kubectl
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>ubuntu@mink8s2:~$ API_IP=10.70.10.228 # set to your original node&amp;#39;s IP
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>ubuntu@mink8s2:~$ cat &amp;lt;&amp;lt;EOS &amp;gt; kubeconfig.yaml
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>apiVersion: v1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>kind: Config
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>clusters:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>- cluster:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> server: http://$API_IP:8080
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> name: mink8s
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>contexts:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>- context:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> cluster: mink8s
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> name: mink8s
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>current-context: mink8s
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>EOS
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>ubuntu@mink8s2:~$ mkdir -p .kube
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>ubuntu@mink8s2:~$ cp kubeconfig.yaml .kube/config&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>
&lt;/div>
&lt;p>
So far the only difference between this and our original node is that we&amp;#39;re
pointing to our original node&amp;#39;s IP instead of &lt;code>127.0.0.1&lt;/code> in our kubeconfig
files. (In general, there isn&amp;#39;t much of a distinction between Kubernetes
&amp;#34;control nodes&amp;#34; and &amp;#34;worker nodes&amp;#34;—basically the &amp;#34;control plane&amp;#34; just means
whatever nodes are running the Kubernetes API server.) Let&amp;#39;s fire up kubelet and
see what happens:&lt;/p>
&lt;div class="src src-text">
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-text" data-lang="text">&lt;span style="display:flex;">&lt;span>ubuntu@mink8s2:~$ sudo ./kubelet --kubeconfig=kubeconfig.yaml&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>
&lt;/div>
&lt;p>
From another terminal, we can try running pods on our new node:&lt;/p>
&lt;div class="src src-text">
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-text" data-lang="text">&lt;span style="display:flex;">&lt;span>ubuntu@mink8s2:~$ cat &amp;lt;&amp;lt;EOS | ./kubectl apply -f -
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>apiVersion: v1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>kind: Pod
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>metadata:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> name: nginx2
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>spec:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> containers:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - image: nginx
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> name: nginx
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> nodeName: mink8s2
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>EOS
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>pod/nginx2 created
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>ubuntu@mink8s2:~$ ./kubectl get po nginx2 -owide
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>nginx2 1/1 Running 0 69s 172.17.0.3 mink8s2 &amp;lt;none&amp;gt; &amp;lt;none&amp;gt;
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>ubuntu@mink8s2:~$ curl -s 172.17.0.3 | head -4
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&amp;lt;!DOCTYPE html&amp;gt;
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&amp;lt;html&amp;gt;
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&amp;lt;head&amp;gt;
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&amp;lt;title&amp;gt;Welcome to nginx!&amp;lt;/title&amp;gt;&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>
&lt;/div>
&lt;p>
So it looks like our second node works. That was easy! End of blog post!&lt;/p>
&lt;/div>
&lt;/div>
&lt;div id="outline-container-headline-2" class="outline-2">
&lt;h2 id="headline-2">
But Networking…
&lt;/h2>
&lt;div id="outline-text-headline-2" class="outline-text-2">
&lt;p>Not so fast! A quick check shows that pod-to-pod networking is not working
across nodes:&lt;/p>
&lt;div class="src src-text">
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-text" data-lang="text">&lt;span style="display:flex;">&lt;span>ubuntu@mink8s2:~$ cat &amp;lt;&amp;lt;EOS | ./kubectl apply -f -
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>apiVersion: v1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>kind: Pod
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>metadata:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> name: curlfail
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>spec:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> containers:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - image: curlimages/curl
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> name: curl
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> command: [&amp;#34;curl&amp;#34;, &amp;#34;172.17.0.3&amp;#34;]
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> nodeName: mink8s
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>EOS
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>pod/curlfail created
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>ubuntu@mink8s2:~$ ./kubectl logs curlfail
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>curl: (7) Couldn&amp;#39;t connect to server&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>
&lt;/div>
&lt;p>
What&amp;#39;s going on here? An odd thing about Kubernetes is that it doesn&amp;#39;t actually
handle networking at all, but instead outsources the configuration to external
plugins. Thus far in our journey, kubelet has been relying on Docker to set up
networking, but Docker doesn&amp;#39;t set up any routing between hosts. We&amp;#39;ll need a
different solution to set up a multi-node cluster.&lt;/p>
&lt;p>
So how should we configure pod-to-pod networking? The &lt;a href="https://kubernetes.io/docs/concepts/cluster-administration/networking/#the-kubernetes-network-model">Kubernetes docs&lt;/a> have a
description of the &amp;#34;networking model&amp;#34;, i.e. the rules that all Kubernetes
clusters are supposed to follow:&lt;/p>
&lt;blockquote>
&lt;p>Kubernetes imposes the following fundamental requirements on any networking
implementation (barring any intentional network segmentation policies):&lt;/p>
&lt;ul>
&lt;li>pods on a node can communicate with all pods on all nodes without NAT&lt;/li>
&lt;li>agents on a node (e.g. system daemons, kubelet) can communicate with all
pods on that node&lt;/li>
&lt;/ul>
&lt;p>….&lt;/p>
&lt;ul>
&lt;li>pods in the host network of a node can communicate with all pods on all
nodes without NAT&lt;/li>
&lt;/ul>
&lt;/blockquote>
&lt;p>
So in order to have a &amp;#34;real Kubernetes cluster&amp;#34;, &amp;#34;everything&amp;#34; needs be able to
communicate with &amp;#34;everything&amp;#34; (more or less). The &amp;#34;without NAT&amp;#34; requirement is a
bit confusing (at least for me), since NAT ends up being important in some
networking implementations, but the important part is that &lt;strong>each pod gets its
own IP address&lt;/strong> that will be valid across the entire cluster (which implies that
IP address overlaps are forbidden).&lt;/p>
&lt;p>
The standard way to set up networking in a Kubernetes cluster is to use a plugin
like &lt;a href="https://www.projectcalico.org/">Calico&lt;/a> or &lt;a href="https://github.com/coreos/flannel">Flannel&lt;/a>. But there&amp;#39;s very little to learn from that! Instead,
we&amp;#39;ll dive into the dark arts of container networking and try to implement the
networking model ourselves.&lt;/p>
&lt;/div>
&lt;/div>
&lt;div id="outline-container-headline-3" class="outline-2">
&lt;h2 id="headline-3">
netns, bridges, and veths, Oh My!
&lt;/h2>
&lt;div id="outline-text-headline-3" class="outline-text-2">
&lt;p>Let&amp;#39;s take a step back and look at how container networking actually
works. (Warning: things are going to get a bit networky from here on out! I&amp;#39;ll
assume you know some networking basics like &lt;a href="https://en.wikipedia.org/wiki/OSI_model">layer 2 vs layer 3&lt;/a> switching/routing
and &lt;a href="https://en.wikipedia.org/wiki/Classless_Inter-Domain_Routing#CIDR_notation">CIDR notation&lt;/a>, but I&amp;#39;ll try to keep things as simple as possible.)&lt;/p>
&lt;p>
The technology at the heart of containerization is &lt;a href="https://en.wikipedia.org/wiki/Linux_namespaces">Linux namespacing&lt;/a>, which
allows for isolation of various resources without full OS-level virtualization.
The kind of namespace we care about here is a &lt;a href="https://man7.org/linux/man-pages/man8/ip-netns.8.html">network namespace&lt;/a> (aka &amp;#34;netns&amp;#34;),
which provides a full copy of the Linux networking stack that&amp;#39;s completely
isolated from the &amp;#34;main&amp;#34; one. Every Kubernetes pod gets its own network
namespace (if there are multiple containers in a pod, they share the same
namespace).&lt;/p>
&lt;p>
A network namespace begins its life as a blank slate with no network devices. In
order for anything useful to happen, the network has to be configured. There are
many ways to configure pod/container networking, but many of them take advantage
of a couple of powerful (and painfully underdocumented) Linux networking
features:&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://www.man7.org/linux/man-pages/man8/bridge.8.html">&lt;strong>bridges&lt;/strong>&lt;/a>: bridges are like virtual network switches that live within the
Linux kernel.&lt;/li>
&lt;li>&lt;a href="https://man7.org/linux/man-pages/man4/veth.4.html">&lt;strong>veths&lt;/strong>&lt;/a>: veths are like virtual network cables that attach two network devices
(physical or virtual). They always come in pairs, one for each end of the
&amp;#34;cable&amp;#34;.&lt;/li>
&lt;/ul>
&lt;p>The steps for setting up networking on a pod look something like:&lt;/p>
&lt;ol>
&lt;li>Add a bridge (typically there will be one per host).&lt;/li>
&lt;li>Create a netns for the pod (there will be one per pod).&lt;/li>
&lt;li>Add a veth pair with one end of the pair in the pod&amp;#39;s netns and the other end
connected to the bridge.&lt;/li>
&lt;li>Assign IP addresses and add routes as necessary.&lt;/li>
&lt;/ol>
&lt;p>Here&amp;#39;s a mediocre diagram of the setup we&amp;#39;re looking for, with a couple of pods
attached to a bridge over veth pairs:&lt;/p>
&lt;p>
&lt;figure>
&lt;img src="https://eevans.co/blog/deconstructing-kubernetes-networking/bridgenet.svg" />
&lt;/figure>
&lt;/p>
&lt;p>
That&amp;#39;s all a bit abstract; I find it easier to understand by actually setting
everything up manually. You can use the &lt;a href="https://man7.org/linux/man-pages/man8/ip.8.html">&lt;code>ip&lt;/code>&lt;/a> command to configure a netns with
the appropriate bridge/veths without having to actually make a container:&lt;/p>
&lt;div class="src src-text">
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-text" data-lang="text">&lt;span style="display:flex;">&lt;span># Create a netns named &amp;#34;test&amp;#34;
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>$ sudo ip netns add test
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span># Create a bridge named &amp;#34;test0&amp;#34;
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>$ sudo ip link add name test0 type bridge
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span># Create a veth pair with testveth0&amp;lt;-&amp;gt;eth0 as the endpoints
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>$ sudo ip link add testveth0 type veth peer name eth0
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span># Move the eth0 side of the veth pair to the &amp;#34;test&amp;#34; netns
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>$ sudo ip link set eth0 netns test
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span># &amp;#34;Plug in&amp;#34; the testveth0 side of the veth pair to the test0 bridge
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>$ sudo ip link set testveth0 master test0
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span># Bring up the testveth0 side of the veth pair
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>$ sudo ip link set testveth0 up
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span># Bring up the eth0 side of the veth pair
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>$ sudo ip -n test link set eth0 up
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span># List network devices in the &amp;#34;main&amp;#34; namespace
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>$ ip link
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>1: lo: &amp;lt;LOOPBACK,UP,LOWER_UP&amp;gt; mtu 65536 qdisc noqueue state UNKNOWN mode DEFAULT group default qlen 1000
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>2: ens3: &amp;lt;BROADCAST,MULTICAST,UP,LOWER_UP&amp;gt; mtu 1450 qdisc fq_codel state UP mode DEFAULT group default qlen 1000
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> link/ether fa:16:3e:cf:81:3d brd ff:ff:ff:ff:ff:ff
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>11: test0: &amp;lt;BROADCAST,MULTICAST,UP,LOWER_UP&amp;gt; mtu 1500 qdisc noqueue state UP mode DEFAULT group default qlen 1000
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> link/ether 4a:13:0d:fb:9f:a4 brd ff:ff:ff:ff:ff:ff
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>15: testveth0@if14: &amp;lt;BROADCAST,MULTICAST,UP,LOWER_UP&amp;gt; mtu 1500 qdisc noqueue master test0 state UP mode DEFAULT group default qlen 1000
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> link/ether 4a:13:0d:fb:9f:a4 brd ff:ff:ff:ff:ff:ff link-netnsid 1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span># List network devices in the &amp;#34;test&amp;#34; namespace
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>$ sudo ip -n test link
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>1: lo: &amp;lt;LOOPBACK&amp;gt; mtu 65536 qdisc noop state DOWN mode DEFAULT group default qlen 1000
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>14: eth0@if15: &amp;lt;BROADCAST,MULTICAST,UP,LOWER_UP&amp;gt; mtu 1500 qdisc noqueue state UP mode DEFAULT group default qlen 1000
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> link/ether a6:4f:92:e2:4b:1e brd ff:ff:ff:ff:ff:ff link-netnsid 0&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>
&lt;/div>
&lt;p>
Don&amp;#39;t worry if you don&amp;#39;t understand all of that &lt;code>ip&lt;/code> output (I&amp;#39;m personally at
about 40% comprehension)—the important thing to note is that the &amp;#34;view&amp;#34; of the
network looks entirely different from within the &lt;code>test&lt;/code> namespace, but bridges
and veths give us a way to communicate across namespaces.&lt;/p>
&lt;p>
How can we get to this setup for our Kubernetes cluster? As I mentioned before,
Kubernetes doesn&amp;#39;t handle any networking itself—instead, it outsources the
configuration to an external plugin. The standard that networking plugins
generally use is called the &lt;a href="https://github.com/containernetworking/cni/">Container Network Interface&lt;/a> (CNI). A CNI plugin is
basically just a binary that follows the &lt;a href="https://github.com/containernetworking/cni/blob/master/SPEC.md">CNI specification&lt;/a>; it has the somewhat
arbitrary job of setting up a container&amp;#39;s network &lt;em>after&lt;/em> the network namespace
has been created but &lt;em>before&lt;/em> the container starts.&lt;/p>
&lt;/div>
&lt;/div>
&lt;div id="outline-container-headline-4" class="outline-2">
&lt;h2 id="headline-4">
Back to Our Node
&lt;/h2>
&lt;div id="outline-text-headline-4" class="outline-text-2">
&lt;p>Enough theory, let&amp;#39;s actually set up CNI! For the rest of this post, we&amp;#39;ll be
working with our original node (&lt;code>mink8s&lt;/code> in my case).&lt;/p>
&lt;p>
First we need to pick an IP range for our pods. I&amp;#39;m going to arbitrarily decide
that my &lt;code>mink8s&lt;/code> node is going to use the &lt;code>10.12.1.0/24&lt;/code> range
(i.e. &lt;code>10.12.1.0&lt;/code> - &lt;code>10.12.1.255&lt;/code>). That gives us more than enough IPs to work
with for our purposes. (When we go multi-node we can give the other nodes in our
cluster similar ranges.)&lt;/p>
&lt;p>
The first thing we&amp;#39;ll have to do (to save many hours of debugging woes) is to
&lt;em>disable Docker&amp;#39;s built-in networking entirely&lt;/em>. For boring historical reasons,
Docker does &lt;em>not&lt;/em> use CNI, and its built-in solution interferes with the setup
we&amp;#39;re going for. Edit &lt;code>/etc/docker/daemon.json&lt;/code> to look like this:&lt;/p>
&lt;div class="src src-javascript">
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-javascript" data-lang="javascript">&lt;span style="display:flex;">&lt;span>{
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#0ff;font-weight:bold">&amp;#34;exec-opts&amp;#34;&lt;/span>: [&lt;span style="color:#0ff;font-weight:bold">&amp;#34;native.cgroupdriver=cgroupfs&amp;#34;&lt;/span>],
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#0ff;font-weight:bold">&amp;#34;log-driver&amp;#34;&lt;/span>: &lt;span style="color:#0ff;font-weight:bold">&amp;#34;json-file&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#0ff;font-weight:bold">&amp;#34;log-opts&amp;#34;&lt;/span>: {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#0ff;font-weight:bold">&amp;#34;max-size&amp;#34;&lt;/span>: &lt;span style="color:#0ff;font-weight:bold">&amp;#34;100m&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> },
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#0ff;font-weight:bold">&amp;#34;bridge&amp;#34;&lt;/span>: &lt;span style="color:#0ff;font-weight:bold">&amp;#34;none&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#0ff;font-weight:bold">&amp;#34;iptables&amp;#34;&lt;/span>: &lt;span style="color:#fff;font-weight:bold">false&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#0ff;font-weight:bold">&amp;#34;ip-masq&amp;#34;&lt;/span>: &lt;span style="color:#fff;font-weight:bold">false&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#0ff;font-weight:bold">&amp;#34;storage-driver&amp;#34;&lt;/span>: &lt;span style="color:#0ff;font-weight:bold">&amp;#34;overlay2&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>}&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>
&lt;/div>
&lt;p>
Most of these settings aren&amp;#39;t important for our purposes, but the &lt;code>bridge&lt;/code>,
&lt;code>iptables&lt;/code>, and &lt;code>ip-masq&lt;/code> options are critical. Once you&amp;#39;ve edited that file,
&lt;em>reboot the machine&lt;/em> to clear out old network settings and iptables
rules. (Trust me, this will make your life much easier! It&amp;#39;s also probably a
good idea to delete any existing pods you have running to avoid confusion.)&lt;/p>
&lt;p>
Now we&amp;#39;ll have to get CNI up and running. We&amp;#39;re going to use the &lt;a href="https://github.com/containernetworking/plugins">example plugins&lt;/a>
provided by the CNI project; by convention, the binaries live in &lt;code>/opt/cni/bin&lt;/code>:&lt;/p>
&lt;div class="src src-text">
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-text" data-lang="text">&lt;span style="display:flex;">&lt;span>$ curl -L https://github.com/containernetworking/plugins/releases/download/v0.8.6/cni-plugins-linux-amd64-v0.8.6.tgz &amp;gt; cni.tgz
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>$ sudo mkdir -p /opt/cni/bin
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>$ sudo tar xzvf cni.tgz -C /opt/cni/bin&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>
&lt;/div>
&lt;p>
Now we&amp;#39;ll make a CNI network configuration file that will use the &lt;a href="https://github.com/containernetworking/plugins/tree/master/plugins/main/bridge">&lt;code>bridge&lt;/code> CNI
plugin&lt;/a>, which sets up networking according to the basic scheme outlined
earlier. Confusingly, to use CNI we actually need to configure two
plugins: a &amp;#34;main&amp;#34; plugin and an &amp;#34;IPAM&amp;#34; plugin (IPAM stands for IP Address
Management). The IPAM plugin is responsible for allocating IPs for pods while
the main plugin does most of the rest of the configuration. We&amp;#39;ll be using the
&lt;a href="https://github.com/containernetworking/plugins/tree/master/plugins/ipam/host-local">&lt;code>host-local&lt;/code> IPAM plugin&lt;/a>, which just allocates IPs from a range and makes sure
there are no overlaps on the host.&lt;/p>
&lt;p>
OK enough theory—let&amp;#39;s take a first crack at a minimal CNI
configuration. Kubelet will look for CNI configuration files in the
&lt;code>/etc/cni/net.d&lt;/code> directory by default. Put the following in
&lt;code>/etc/cni/net.d/mink8s.conf&lt;/code>:&lt;/p>
&lt;div class="src src-javascript">
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-javascript" data-lang="javascript">&lt;span style="display:flex;">&lt;span>{
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#0ff;font-weight:bold">&amp;#34;cniVersion&amp;#34;&lt;/span>: &lt;span style="color:#0ff;font-weight:bold">&amp;#34;0.3.1&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#0ff;font-weight:bold">&amp;#34;name&amp;#34;&lt;/span>: &lt;span style="color:#0ff;font-weight:bold">&amp;#34;mink8s&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#0ff;font-weight:bold">&amp;#34;type&amp;#34;&lt;/span>: &lt;span style="color:#0ff;font-weight:bold">&amp;#34;bridge&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#0ff;font-weight:bold">&amp;#34;bridge&amp;#34;&lt;/span>: &lt;span style="color:#0ff;font-weight:bold">&amp;#34;mink8s0&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#0ff;font-weight:bold">&amp;#34;ipam&amp;#34;&lt;/span>: {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#0ff;font-weight:bold">&amp;#34;type&amp;#34;&lt;/span>: &lt;span style="color:#0ff;font-weight:bold">&amp;#34;host-local&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#0ff;font-weight:bold">&amp;#34;ranges&amp;#34;&lt;/span>: [
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> [{&lt;span style="color:#0ff;font-weight:bold">&amp;#34;subnet&amp;#34;&lt;/span>: &lt;span style="color:#0ff;font-weight:bold">&amp;#34;10.12.1.0/24&amp;#34;&lt;/span>}]
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> ]
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> }
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>}&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>
&lt;/div>
&lt;p>
To dissect that configuration a bit:&lt;/p>
&lt;ul>
&lt;li>&lt;code>type&lt;/code> and &lt;code>ipam.type&lt;/code> specify the actual plugin binary names (so it will look
for &lt;code>/opt/cni/bin/bridge&lt;/code> and &lt;code>/opt/cni/bin/host-local&lt;/code> for the plugins we&amp;#39;re
using).&lt;/li>
&lt;li>&lt;code>bridge&lt;/code> specifies the name of the network bridge that the &lt;code>bridge&lt;/code> plugin
will create.&lt;/li>
&lt;li>&lt;code>ipam.ranges&lt;/code> specifies the IP ranges to allocate to pods. In our case, we&amp;#39;re
going to allocate IPs in the &lt;code>10.12.1.0/24&lt;/code> range.&lt;/li>
&lt;/ul>
&lt;p>Now we&amp;#39;ll restart kubelet and pass the &lt;code>network-plugin=cni&lt;/code> option:&lt;/p>
&lt;div class="src src-text">
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-text" data-lang="text">&lt;span style="display:flex;">&lt;span>$ sudo ./kubelet --network-plugin=cni --pod-manifest-path=pods --kubeconfig=kubeconfig.yaml&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>
&lt;/div>
&lt;p>
And then we&amp;#39;ll create two &amp;#34;sleeping&amp;#34; pods to see if networking actually works:&lt;/p>
&lt;div class="src src-text">
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-text" data-lang="text">&lt;span style="display:flex;">&lt;span>$ for i in 1 2; do cat &amp;lt;&amp;lt;EOS | ./kubectl apply -f - ; done
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>---
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>apiVersion: v1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>kind: Pod
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>metadata:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> name: sleep${i}
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>spec:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> containers:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - image: alpine
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> name: alpine
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> command: [&amp;#34;sleep&amp;#34;, &amp;#34;5000000&amp;#34;]
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> nodeName: mink8s
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>EOS&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>
&lt;/div>
&lt;p>
Some poking around shows that both pods get IP addresses and can ping each
other, which is a great first step!&lt;/p>
&lt;div class="src src-text">
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-text" data-lang="text">&lt;span style="display:flex;">&lt;span>$ ./kubectl get po -owide
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>sleep1 1/1 Running 0 7s 10.12.1.4 mink8s &amp;lt;none&amp;gt; &amp;lt;none&amp;gt;
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>sleep2 1/1 Running 0 6s 10.12.1.5 mink8s &amp;lt;none&amp;gt; &amp;lt;none&amp;gt;
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>$ ./kubectl exec sleep1 -- ping 10.12.1.5
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>PING 10.12.1.5 (10.12.1.5): 56 data bytes
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>64 bytes from 10.12.1.5: seq=0 ttl=64 time=0.627 ms
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>64 bytes from 10.12.1.5: seq=1 ttl=64 time=0.075 ms
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>64 bytes from 10.12.1.5: seq=2 ttl=64 time=0.116 ms&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>
&lt;/div>
&lt;p>
Some more poking around shows that the &lt;code>bridge&lt;/code> plugin has indeed created a
bridge named &lt;code>mink8s0&lt;/code> as well as a veth pair for each pod:&lt;/p>
&lt;div class="src src-text">
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-text" data-lang="text">&lt;span style="display:flex;">&lt;span>$ ip link
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>1: lo: &amp;lt;LOOPBACK,UP,LOWER_UP&amp;gt; mtu 65536 qdisc noqueue state UNKNOWN mode DEFAULT group default qlen 1000
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>2: ens3: &amp;lt;BROADCAST,MULTICAST,UP,LOWER_UP&amp;gt; mtu 1450 qdisc fq_codel state UP mode DEFAULT group default qlen 1000
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> link/ether fa:16:3e:cf:81:3d brd ff:ff:ff:ff:ff:ff
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>3: mink8s0: &amp;lt;BROADCAST,MULTICAST,UP,LOWER_UP&amp;gt; mtu 1500 qdisc noqueue state UP mode DEFAULT group default qlen 1000
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> link/ether 46:ee:b5:e0:67:a4 brd ff:ff:ff:ff:ff:ff
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>6: veth19e99be3@if3: &amp;lt;BROADCAST,MULTICAST,UP,LOWER_UP&amp;gt; mtu 1500 qdisc noqueue master mink8s0 state UP mode DEFAULT group default
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> link/ether 46:ee:b5:e0:67:a4 brd ff:ff:ff:ff:ff:ff link-netnsid 0
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>7: veth5947e6fb@if3: &amp;lt;BROADCAST,MULTICAST,UP,LOWER_UP&amp;gt; mtu 1500 qdisc noqueue master mink8s0 state UP mode DEFAULT group default
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> link/ether b2:b6:d4:49:fb:b9 brd ff:ff:ff:ff:ff:ff link-netnsid 1&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>
&lt;/div>
&lt;p>
(Annoyingly, kubelet creates the network namespaces in such a way that they
don&amp;#39;t show up in &lt;code>ip netns&lt;/code>. But the &lt;code>link-netnsid&lt;/code> attribute gives a hint that
the veths are indeed connected to veths in other namespaces.)&lt;/p>
&lt;p>
We&amp;#39;re still a ways off from implementing our full Kubernetes network model,
however. Pinging the pods from the host doesn&amp;#39;t work (which you may remember is
a requirement of the model), and neither does pinging the host from the pods
(which I don&amp;#39;t think is a strict requirement in theory but is going to be
essential in practice):&lt;/p>
&lt;div class="src src-text">
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-text" data-lang="text">&lt;span style="display:flex;">&lt;span>$ HOST_IP=10.70.10.228 # set to whatever your host&amp;#39;s internal IP address is
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>$ ping 10.12.1.4
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>PING 10.12.1.4 (10.12.1.4) 56(84) bytes of data.
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>^C
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>--- 10.12.1.4 ping statistics ---
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>3 packets transmitted, 0 received, 100% packet loss, time 2047ms
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>$ ./kubectl exec sleep1 -- ping $HOST_IP
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>PING 10.70.10.228 (10.70.10.228): 56 data bytes
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>ping: sendto: Network unreachable
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>command terminated with exit code 1&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>
&lt;/div>
&lt;p>
The reason the host and pods can&amp;#39;t communicate with each other is that they&amp;#39;re
on different network subnets (in my case, &lt;code>10.12.1.0/24&lt;/code> for the pods and
&lt;code>10.70.0.0/16&lt;/code> for the VM), which means they can&amp;#39;t communicate directly over
Ethernet and will need to use IP routing to find each other (for the
networking-jargon-inclined: we need to go from layer 2 to layer 3). Linux
bridges work on layer 2 by default, but can actually handle layer 3 routing just
fine if you assign IP addresses to them. (You can confirm that the bridge
doesn&amp;#39;t currently have an IP address with &lt;code>ip addr show dev mink8s0&lt;/code>.)&lt;/p>
&lt;p>
To configure the bridge to use layer 3 routing, we&amp;#39;ll set the &lt;code>isGateway&lt;/code> option
in our CNI config file. Here&amp;#39;s our next attempt at the configuration:&lt;/p>
&lt;div class="src src-javascript">
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-javascript" data-lang="javascript">&lt;span style="display:flex;">&lt;span>{
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#0ff;font-weight:bold">&amp;#34;cniVersion&amp;#34;&lt;/span>: &lt;span style="color:#0ff;font-weight:bold">&amp;#34;0.3.1&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#0ff;font-weight:bold">&amp;#34;name&amp;#34;&lt;/span>: &lt;span style="color:#0ff;font-weight:bold">&amp;#34;mink8s&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#0ff;font-weight:bold">&amp;#34;type&amp;#34;&lt;/span>: &lt;span style="color:#0ff;font-weight:bold">&amp;#34;bridge&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#0ff;font-weight:bold">&amp;#34;bridge&amp;#34;&lt;/span>: &lt;span style="color:#0ff;font-weight:bold">&amp;#34;mink8s0&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#0ff;font-weight:bold">&amp;#34;isGateway&amp;#34;&lt;/span>: &lt;span style="color:#fff;font-weight:bold">true&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#0ff;font-weight:bold">&amp;#34;ipam&amp;#34;&lt;/span>: {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#0ff;font-weight:bold">&amp;#34;type&amp;#34;&lt;/span>: &lt;span style="color:#0ff;font-weight:bold">&amp;#34;host-local&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#0ff;font-weight:bold">&amp;#34;ranges&amp;#34;&lt;/span>: [
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> [{&lt;span style="color:#0ff;font-weight:bold">&amp;#34;subnet&amp;#34;&lt;/span>: &lt;span style="color:#0ff;font-weight:bold">&amp;#34;10.12.1.0/24&amp;#34;&lt;/span>}]
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> ]
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> }
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>}&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>
&lt;/div>
&lt;p>
Whenever we change the CNI configuration, we&amp;#39;ll want to delete and recreate all
our pods, since the networking configuration is only used on pod
creation/deletion. Once we do that, we find that the bridge has been given an IP
address and we can ping the pods from the host, but pinging the host from the
pods still doesn&amp;#39;t work:&lt;/p>
&lt;div class="src src-text">
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-text" data-lang="text">&lt;span style="display:flex;">&lt;span>$ ip addr show dev mink8s0 | grep 10.12
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> inet 10.12.1.1/24 brd 10.12.1.255 scope global mink8s0
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>$ ./kubectl get po -owide
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>sleep1 1/1 Running 0 5m56s 10.12.1.8 mink8s &amp;lt;none&amp;gt; &amp;lt;none&amp;gt;
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>sleep2 1/1 Running 0 5m55s 10.12.1.7 mink8s &amp;lt;none&amp;gt; &amp;lt;none&amp;gt;
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>$ ping -c 3 10.12.1.8
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>PING 10.12.1.8 (10.12.1.8) 56(84) bytes of data.
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>64 bytes from 10.12.1.8: icmp_seq=1 ttl=64 time=0.063 ms
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>64 bytes from 10.12.1.8: icmp_seq=2 ttl=64 time=0.087 ms
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>64 bytes from 10.12.1.8: icmp_seq=3 ttl=64 time=0.099 ms
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>$ ./kubectl exec sleep1 -- ping $HOST_IP
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>PING 10.70.10.228 (10.70.10.228): 56 data bytes
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>ping: sendto: Network unreachable
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>command terminated with exit code 1&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>
&lt;/div>
&lt;p>
The reason it&amp;#39;s still not working is that the pod doesn&amp;#39;t have a default route
set up (you can confirm this with &lt;code>./kubectl exec sleep1 -- ip route&lt;/code>). We can
solve this problem by adding a default route in our CNI config. Let&amp;#39;s add a
route to our configuration to &lt;code>0.0.0.0/0&lt;/code> (i.e. everywhere):&lt;/p>
&lt;div class="src src-javascript">
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-javascript" data-lang="javascript">&lt;span style="display:flex;">&lt;span>{
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#0ff;font-weight:bold">&amp;#34;cniVersion&amp;#34;&lt;/span>: &lt;span style="color:#0ff;font-weight:bold">&amp;#34;0.3.1&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#0ff;font-weight:bold">&amp;#34;name&amp;#34;&lt;/span>: &lt;span style="color:#0ff;font-weight:bold">&amp;#34;mink8s&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#0ff;font-weight:bold">&amp;#34;type&amp;#34;&lt;/span>: &lt;span style="color:#0ff;font-weight:bold">&amp;#34;bridge&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#0ff;font-weight:bold">&amp;#34;bridge&amp;#34;&lt;/span>: &lt;span style="color:#0ff;font-weight:bold">&amp;#34;mink8s0&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#0ff;font-weight:bold">&amp;#34;isGateway&amp;#34;&lt;/span>: &lt;span style="color:#fff;font-weight:bold">true&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#0ff;font-weight:bold">&amp;#34;ipam&amp;#34;&lt;/span>: {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#0ff;font-weight:bold">&amp;#34;type&amp;#34;&lt;/span>: &lt;span style="color:#0ff;font-weight:bold">&amp;#34;host-local&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#0ff;font-weight:bold">&amp;#34;ranges&amp;#34;&lt;/span>: [
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> [{&lt;span style="color:#0ff;font-weight:bold">&amp;#34;subnet&amp;#34;&lt;/span>: &lt;span style="color:#0ff;font-weight:bold">&amp;#34;10.12.1.0/24&amp;#34;&lt;/span>}]
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> ],
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#0ff;font-weight:bold">&amp;#34;routes&amp;#34;&lt;/span>: [{&lt;span style="color:#0ff;font-weight:bold">&amp;#34;dst&amp;#34;&lt;/span>: &lt;span style="color:#0ff;font-weight:bold">&amp;#34;0.0.0.0/0&amp;#34;&lt;/span>}]
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> }
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>}&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>
&lt;/div>
&lt;p>
(For reasons I don&amp;#39;t entirely understand, setting up routes is the
responsibility of the IPAM plugin instead of the &lt;code>bridge&lt;/code> plugin.) Once that&amp;#39;s
saved and our pods have been killed and recreated, we see the default route is
set up and pinging the host works fine:&lt;/p>
&lt;div class="src src-text">
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-text" data-lang="text">&lt;span style="display:flex;">&lt;span>$ ./kubectl exec sleep1 -- ip route
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>default via 10.12.1.1 dev eth0
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>10.12.1.0/24 dev eth0 scope link src 10.12.1.9
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>$ ./kubectl exec sleep1 -- ping -c3 $HOST_IP
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>PING 10.70.10.228 (10.70.10.228): 56 data bytes
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>64 bytes from 10.70.10.228: seq=0 ttl=64 time=0.110 ms
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>64 bytes from 10.70.10.228: seq=1 ttl=64 time=0.269 ms
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>64 bytes from 10.70.10.228: seq=2 ttl=64 time=0.233 ms&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>
&lt;/div>
&lt;p>
Our pods can now talk to each other (on the same node) and the host and pods can
also talk to each other. So technically you could say we&amp;#39;ve implemented the
Kubernetes networking model for one node. But there&amp;#39;s still a glaring omission,
which we&amp;#39;ll see if we try to ping an address outside of our network:&lt;/p>
&lt;div class="src src-text">
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-text" data-lang="text">&lt;span style="display:flex;">&lt;span>$ ./kubectl exec sleep1 -- ping -c3 1.1.1.1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>PING 1.1.1.1 (1.1.1.1): 56 data bytes
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>--- 1.1.1.1 ping statistics ---
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>3 packets transmitted, 0 packets received, 100% packet loss
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>command terminated with exit code 1&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>
&lt;/div>
&lt;p>
Our pods can&amp;#39;t reach the Internet! This isn&amp;#39;t particularly surprising, since our
pods are connected to the bridge network, &lt;em>not&lt;/em> the actual Ethernet adapter of
the host.&lt;/p>
&lt;p>
To get outgoing Internet connectivity working, we&amp;#39;ll need to set up NAT using
the &lt;a href="https://tldp.org/HOWTO/IP-Masquerade-HOWTO/ipmasq-background2.1.html">IP masquerade&lt;/a> feature of iptables. (NAT is necessary in this case because
all of our pods are going to share the external IP address of our host.) The
&lt;code>bridge&lt;/code> plugin has us covered with the &lt;code>ipMasq&lt;/code> option. Let&amp;#39;s save our final
(for this blog) CNI configuration:&lt;/p>
&lt;div class="src src-javascript">
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-javascript" data-lang="javascript">&lt;span style="display:flex;">&lt;span>{
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#0ff;font-weight:bold">&amp;#34;cniVersion&amp;#34;&lt;/span>: &lt;span style="color:#0ff;font-weight:bold">&amp;#34;0.3.1&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#0ff;font-weight:bold">&amp;#34;name&amp;#34;&lt;/span>: &lt;span style="color:#0ff;font-weight:bold">&amp;#34;mink8s&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#0ff;font-weight:bold">&amp;#34;type&amp;#34;&lt;/span>: &lt;span style="color:#0ff;font-weight:bold">&amp;#34;bridge&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#0ff;font-weight:bold">&amp;#34;bridge&amp;#34;&lt;/span>: &lt;span style="color:#0ff;font-weight:bold">&amp;#34;mink8s0&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#0ff;font-weight:bold">&amp;#34;isGateway&amp;#34;&lt;/span>: &lt;span style="color:#fff;font-weight:bold">true&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#0ff;font-weight:bold">&amp;#34;ipMasq&amp;#34;&lt;/span>: &lt;span style="color:#fff;font-weight:bold">true&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#0ff;font-weight:bold">&amp;#34;ipam&amp;#34;&lt;/span>: {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#0ff;font-weight:bold">&amp;#34;type&amp;#34;&lt;/span>: &lt;span style="color:#0ff;font-weight:bold">&amp;#34;host-local&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#0ff;font-weight:bold">&amp;#34;ranges&amp;#34;&lt;/span>: [
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> [{&lt;span style="color:#0ff;font-weight:bold">&amp;#34;subnet&amp;#34;&lt;/span>: &lt;span style="color:#0ff;font-weight:bold">&amp;#34;10.12.1.0/24&amp;#34;&lt;/span>}]
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> ],
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#0ff;font-weight:bold">&amp;#34;routes&amp;#34;&lt;/span>: [{&lt;span style="color:#0ff;font-weight:bold">&amp;#34;dst&amp;#34;&lt;/span>: &lt;span style="color:#0ff;font-weight:bold">&amp;#34;0.0.0.0/0&amp;#34;&lt;/span>}]
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> }
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>}&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>
&lt;/div>
&lt;p>
Once that&amp;#39;s applied, our pods can reach the Internet:&lt;/p>
&lt;div class="src src-text">
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-text" data-lang="text">&lt;span style="display:flex;">&lt;span>$ ./kubectl exec sleep1 -- ping -c3 1.1.1.1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>PING 1.1.1.1 (1.1.1.1): 56 data bytes
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>64 bytes from 1.1.1.1: seq=0 ttl=51 time=4.343 ms
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>64 bytes from 1.1.1.1: seq=1 ttl=51 time=4.189 ms
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>64 bytes from 1.1.1.1: seq=2 ttl=51 time=4.285 ms&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>
&lt;/div>
&lt;p>
We can see the IP masquerade rules created by the plugin by poking around with
&lt;code>iptables&lt;/code>:&lt;/p>
&lt;div class="src src-text">
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-text" data-lang="text">&lt;span style="display:flex;">&lt;span>$ sudo iptables --list POSTROUTING --numeric --table nat
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>Chain POSTROUTING (policy ACCEPT)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>target prot opt source destination
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>KUBE-POSTROUTING all -- 0.0.0.0/0 0.0.0.0/0 /* kubernetes postrouting rules */
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>CNI-c07db3c8c34133af9e525bf4 all -- 10.12.1.11 0.0.0.0/0 /* name: &amp;#34;mink8s&amp;#34; id: &amp;#34;1793c831da4a054beebc6c4dad02088bb7bd4553d435972b7581cb135d349113&amp;#34; */
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>CNI-a874815f36fc490c823cf894 all -- 10.12.1.12 0.0.0.0/0 /* name: &amp;#34;mink8s&amp;#34; id: &amp;#34;f98855905b1b070f7aa7387c844308d53fbeeeba65a23a075cfe6f12ea516005&amp;#34; */
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>$ sudo iptables -L CNI-c07db3c8c34133af9e525bf4 -n -t nat
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>Chain CNI-c07db3c8c34133af9e525bf4 (1 references)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>target prot opt source destination
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>ACCEPT all -- 0.0.0.0/0 10.12.1.0/24 /* name: &amp;#34;mink8s&amp;#34; id: &amp;#34;1793c831da4a054beebc6c4dad02088bb7bd4553d435972b7581cb135d349113&amp;#34; */
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>MASQUERADE all -- 0.0.0.0/0 !224.0.0.0/4 /* name: &amp;#34;mink8s&amp;#34; id: &amp;#34;1793c831da4a054beebc6c4dad02088bb7bd4553d435972b7581cb135d349113&amp;#34; */&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>
&lt;/div>
&lt;p>
For those not fluent in iptablesese, here&amp;#39;s a rough translation of these rules:&lt;/p>
&lt;ul>
&lt;li>If a packet comes from a pod IP address, use a special iptables chain for that
pod (e.g. in this example, &lt;code>10.12.1.11&lt;/code> uses the
&lt;code>CNI-c07db3c8c34133af9e525bf4&lt;/code> chain).&lt;/li>
&lt;li>In that chain, if the packet isn&amp;#39;t going to the pod&amp;#39;s local network or a
special multicast address (the &lt;code>224.0.0.0/4&lt;/code> business), masquerade it.&lt;/li>
&lt;/ul>
&lt;/div>
&lt;/div>
&lt;div id="outline-container-headline-5" class="outline-2">
&lt;h2 id="headline-5">
Phew!
&lt;/h2>
&lt;div id="outline-text-headline-5" class="outline-text-2">
&lt;p>OK that got pretty long and complicated, so I think I&amp;#39;m going to call it a blog
post. We more or less ended up where we started (networking working on a single
Kubernetes node), but by switching from Docker to CNI-based networking we&amp;#39;re in
a good place to get multi-node networking working. And hopefully we learned
something along the way!&lt;/p>
&lt;p>
Next time, we&amp;#39;ll try to get a multi-node cluster up and running!&lt;/p>
&lt;/div>
&lt;/div></description></item><item><title>Minimum Viable Kubernetes</title><link>https://eevans.co/blog/minimum-viable-kubernetes/</link><pubDate>Sun, 05 Jul 2020 00:00:00 +0000</pubDate><guid>https://eevans.co/blog/minimum-viable-kubernetes/</guid><category>kubernetes</category><category>k8s</category><category>technical</category><description>
&lt;p>
If you&amp;#39;re reading this, chances are good that you&amp;#39;ve heard of Kubernetes. (If
you haven&amp;#39;t, how exactly did you end up here?) But what actually is Kubernetes?
Is it &lt;a href="https://kubernetes.io/">&amp;#34;Production-Grade Container Orchestration&amp;#34;&lt;/a>? Is it a &lt;a href="https://platform9.com/blog/kubernetes-as-a-cloud-native-operating-system-on-premises-too/">&amp;#34;Cloud-Native
Operating System&amp;#34;&lt;/a>? What do either of those phrases even mean?&lt;/p>
&lt;p>
To be completely honest, I&amp;#39;m not always 100% sure. But I think it&amp;#39;s interesting
and informative to take a peek under the hood and see what Kubernetes actually
&lt;em>does&lt;/em> under the many layers of abstraction and indirection. So just for fun,
let&amp;#39;s see what the absolute bare minimum &amp;#34;Kubernetes cluster&amp;#34; actually looks
like. (It&amp;#39;s going to be a lot more minimal than setting up Kubernetes &lt;a href="https://github.com/kelseyhightower/kubernetes-the-hard-way">the hard
way&lt;/a>.)&lt;/p>
&lt;p>
I&amp;#39;m going to assume a basic familiarity with Kubernetes, Linux, and containers,
but nothing too advanced. By the way, this is all for learning/exploration
purposes, so don&amp;#39;t run any of it in production!&lt;/p>
&lt;div id="outline-container-headline-1" class="outline-2">
&lt;h2 id="headline-1">
Big Picture View
&lt;/h2>
&lt;div id="outline-text-headline-1" class="outline-text-2">
&lt;p>Kubernetes has a lot of components and it&amp;#39;s sometimes a bit difficult to keep
track of all of them. Here&amp;#39;s what the overall architecture looks like according
to &lt;a href="https://commons.wikimedia.org/w/index.php?curid=53571935">Wikipedia&lt;/a>:&lt;/p>
&lt;p>
&lt;figure>
&lt;img src="https://eevans.co/blog/minimum-viable-kubernetes/k8s-arch.png" />
&lt;/figure>
&lt;/p>
&lt;p>
There are at least eight components listed in that diagram; we&amp;#39;re going to be
ignoring most of them. I&amp;#39;m going to make the claim that the minimal thing you
could reasonably call Kubernetes consists of three essential components:&lt;/p>
&lt;ul>
&lt;li>kubelet&lt;/li>
&lt;li>kube-apiserver (which depends on etcd as its database)&lt;/li>
&lt;li>A container runtime (Docker in this case)&lt;/li>
&lt;/ul>
&lt;p>Let&amp;#39;s take a closer look at what each of these do, according to &lt;a href="https://kubernetes.io/docs/concepts/overview/components/">the docs&lt;/a>. First,
&lt;strong>kubelet&lt;/strong>:&lt;/p>
&lt;blockquote>
&lt;p>An agent that runs on each node in the cluster. It makes sure that containers
are running in a Pod.&lt;/p>
&lt;/blockquote>
&lt;p>
That sounds simple enough. What about the &lt;strong>container runtime&lt;/strong>?&lt;/p>
&lt;blockquote>
&lt;p>The container runtime is the software that is responsible for running
containers.&lt;/p>
&lt;/blockquote>
&lt;p>
Tremendously informative. But if you&amp;#39;re familiar with Docker, than you should
have a basic idea of what it does. (The details of the separation of concerns
between the container runtime and kubelet are actually a bit subtle, but I won&amp;#39;t
be digging into them here.)&lt;/p>
&lt;p>
And the &lt;strong>API server&lt;/strong>?&lt;/p>
&lt;blockquote>
&lt;p>The API server is a component of the Kubernetes control plane that exposes the
Kubernetes API. The API server is the front end for the Kubernetes control
plane.&lt;/p>
&lt;/blockquote>
&lt;p>
Anyone who&amp;#39;s ever done anything with Kubernetes has interacted with the API,
either directly or through kubectl. It&amp;#39;s the core of what makes Kubernetes
Kubernetes, the brain that turns the mountains of YAML we all know and love (?)
into running infrastructure. It seems obvious that we&amp;#39;ll want to get it running
for our minimal setup.&lt;/p>
&lt;/div>
&lt;/div>
&lt;div id="outline-container-headline-2" class="outline-2">
&lt;h2 id="headline-2">
Prerequisites If You Want to Follow Along
&lt;/h2>
&lt;div id="outline-text-headline-2" class="outline-text-2">
&lt;ul>
&lt;li>A Linux virtual or corporeal machine you&amp;#39;re OK messing around with as
root (I&amp;#39;m using Ubuntu 18.04 on a VM).&lt;/li>
&lt;li>That&amp;#39;s it!&lt;/li>
&lt;/ul>
&lt;/div>
&lt;/div>
&lt;div id="outline-container-headline-3" class="outline-2">
&lt;h2 id="headline-3">
The Boring Setup
&lt;/h2>
&lt;div id="outline-text-headline-3" class="outline-text-2">
&lt;p>
The machine we&amp;#39;re using needs Docker installed. (I&amp;#39;m not going to dig too much
into how Docker and containers work; there are some &lt;a href="https://blog.lizzie.io/linux-containers-in-500-loc.html">amazing rabbit holes&lt;/a> already
out there if you&amp;#39;re interested.) Let&amp;#39;s just install it using &lt;code>apt&lt;/code>:&lt;/p>
&lt;div class="src src-text">
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-text" data-lang="text">&lt;span style="display:flex;">&lt;span>$ sudo apt install docker.io
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>$ sudo systemctl start docker&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>
&lt;/div>
&lt;p>
Next we&amp;#39;ll need to get the Kubernetes binaries. We actually only need kubelet to
bootstrap our &amp;#34;cluster&amp;#34;, since we can use kubelet to run the other server
components. We&amp;#39;ll also grab kubectl to interact with our cluster once it&amp;#39;s up
and running.&lt;/p>
&lt;div class="src src-text">
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-text" data-lang="text">&lt;span style="display:flex;">&lt;span>$ curl -L https://dl.k8s.io/v1.18.5/kubernetes-server-linux-amd64.tar.gz &amp;gt; server.tar.gz
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>$ tar xzvf server.tar.gz
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>$ cp kubernetes/server/bin/kubelet .
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>$ cp kubernetes/server/bin/kubectl .
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>$ ./kubelet --version
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>Kubernetes v1.18.5&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>
&lt;/div>
&lt;/div>
&lt;/div>
&lt;div id="outline-container-headline-4" class="outline-2">
&lt;h2 id="headline-4">
Off To The Races
&lt;/h2>
&lt;div id="outline-text-headline-4" class="outline-text-2">
&lt;p>
What happens when we try to run kubelet?&lt;/p>
&lt;div class="src src-text">
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-text" data-lang="text">&lt;span style="display:flex;">&lt;span>$ ./kubelet
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>F0609 04:03:29.105194 4583 server.go:254] mkdir /var/lib/kubelet: permission denied&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>
&lt;/div>
&lt;p>
kubelet needs to run as root; fair enough, since it&amp;#39;s tasked with managing the
entire node. Let&amp;#39;s see what the CLI options look like:&lt;/p>
&lt;div class="src src-text">
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-text" data-lang="text">&lt;span style="display:flex;">&lt;span>$ ./kubelet -h
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&amp;lt;far too much output to copy here&amp;gt;
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>$ ./kubelet -h | wc -l
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>284&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>
&lt;/div>
&lt;p>
Holy cow, that&amp;#39;s a lot of options! Thankfully we&amp;#39;ll only need a couple of them
for our setup. Here&amp;#39;s an option that looks kind of interesting:&lt;/p>
&lt;blockquote>
&lt;p>&lt;code>--pod-manifest-path&lt;/code> string&lt;/p>
&lt;p>
Path to the directory containing static pod files to run, or the path to a
single static pod file. Files starting with dots will be ignored. (DEPRECATED:
This parameter should be set via the config file specified by the Kubelet&amp;#39;s
–config flag. See
&lt;a href="https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/">https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/&lt;/a> for
more information.)&lt;/p>
&lt;/blockquote>
&lt;p>
This option allows us to run &lt;a href="https://kubernetes.io/docs/tasks/configure-pod-container/static-pod/">static pods&lt;/a>, which are pods that aren&amp;#39;t managed
through the Kubernetes API. Static pods aren&amp;#39;t that common in day-to-day
Kubernetes usage but are very useful for bootstrapping clusters, which is
exactly what we&amp;#39;re trying to do here. We&amp;#39;re going to ignore the loud deprecation
warning (again, don&amp;#39;t run this in prod!) and see if we can run a pod.&lt;/p>
&lt;p>
First we&amp;#39;ll make a static pod directory and run kubelet:&lt;/p>
&lt;div class="src src-text">
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-text" data-lang="text">&lt;span style="display:flex;">&lt;span>$ mkdir pods
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>$ sudo ./kubelet --pod-manifest-path=pods&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>
&lt;/div>
&lt;p>
Then, in another terminal/tmux window/whatever, we&amp;#39;ll make a pod manifest:&lt;/p>
&lt;div class="src src-text">
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-text" data-lang="text">&lt;span style="display:flex;">&lt;span>$ cat &amp;lt;&amp;lt;EOF &amp;gt; pods/hello.yaml
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>apiVersion: v1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>kind: Pod
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>metadata:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> name: hello
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>spec:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> containers:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - image: busybox
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> name: hello
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> command: [&amp;#34;echo&amp;#34;, &amp;#34;hello world!&amp;#34;]
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>EOF&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>
&lt;/div>
&lt;p>
kubelet starts spitting out some warnings; other than that it anti-climatically
appears that nothing really happened. But not so! Let&amp;#39;s check Docker:&lt;/p>
&lt;div class="src src-text">
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-text" data-lang="text">&lt;span style="display:flex;">&lt;span>$ sudo docker ps -a
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>8c8a35e26663 busybox &amp;#34;echo &amp;#39;hello world!&amp;#39;&amp;#34; 36 seconds ago Exited (0) 36 seconds ago k8s_hello_hello-mink8s_default_ab61ef0307c6e0dee2ab05dc1ff94812_4
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>68f670c3c85f k8s.gcr.io/pause:3.2 &amp;#34;/pause&amp;#34; 2 minutes ago Up 2 minutes k8s_POD_hello-mink8s_default_ab61ef0307c6e0dee2ab05dc1ff94812_0
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>$ sudo docker logs k8s_hello_hello-mink8s_default_ab61ef0307c6e0dee2ab05dc1ff94812_4
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>hello world!&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>
&lt;/div>
&lt;p>
kubelet read the pod manifest and instructed Docker to start a couple of
containers according to our specification. (If you&amp;#39;re wondering about that
&amp;#34;pause&amp;#34; container, it&amp;#39;s Kubernetes hackery that&amp;#39;s used to reap zombie processes
—see &lt;a href="https://www.ianlewis.org/en/almighty-pause-container">this blog post&lt;/a> for the gory details.) kubelet will run our &lt;code>busybox&lt;/code>
container with our command and restart it ad infinitum until the static pod is
removed.&lt;/p>
&lt;p>
Let&amp;#39;s congratulate ourselves: we&amp;#39;ve just figured out one of the world&amp;#39;s most
convoluted ways of printing out text to the terminal!&lt;/p>
&lt;/div>
&lt;/div>
&lt;div id="outline-container-headline-5" class="outline-2">
&lt;h2 id="headline-5">
Getting etcd running
&lt;/h2>
&lt;div id="outline-text-headline-5" class="outline-text-2">
&lt;p>
Our eventual goal is to run the Kubernetes API, but in order to do that we&amp;#39;ll
need &lt;a href="https://etcd.io/">etcd&lt;/a> running first. A static pod ought to fit the bill. Let&amp;#39;s run a minimal
etcd cluster by putting the following in a file in the &lt;code>pods&lt;/code> directory
(e.g. &lt;code>pods/etcd.yaml&lt;/code>):&lt;/p>
&lt;div class="src src-yaml">
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>&lt;span style="font-weight:bold">apiVersion&lt;/span>: v1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="font-weight:bold">kind&lt;/span>: Pod
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="font-weight:bold">metadata&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="font-weight:bold">name&lt;/span>: etcd
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="font-weight:bold">namespace&lt;/span>: kube-system
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="font-weight:bold">spec&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="font-weight:bold">containers&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - &lt;span style="font-weight:bold">name&lt;/span>: etcd
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="font-weight:bold">command&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - etcd
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - --data-dir=/var/lib/etcd
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="font-weight:bold">image&lt;/span>: k8s.gcr.io/etcd:3.4.3-0
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="font-weight:bold">volumeMounts&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - &lt;span style="font-weight:bold">mountPath&lt;/span>: /var/lib/etcd
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="font-weight:bold">name&lt;/span>: etcd-data
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="font-weight:bold">hostNetwork&lt;/span>: &lt;span style="color:#fff;font-weight:bold">true&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="font-weight:bold">volumes&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - &lt;span style="font-weight:bold">hostPath&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="font-weight:bold">path&lt;/span>: /var/lib/etcd
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="font-weight:bold">type&lt;/span>: DirectoryOrCreate
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="font-weight:bold">name&lt;/span>: etcd-data&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>
&lt;/div>
&lt;p>
If you&amp;#39;ve ever worked with Kubernetes, this kind of YAML file should look
familiar. There are only two slightly unusual things worth noting:&lt;/p>
&lt;ul>
&lt;li>We mounted the host&amp;#39;s &lt;code>/var/lib/etcd&lt;/code> to the pod so that the etcd data will
survive restarts (if we didn&amp;#39;t do this the cluster state would get wiped every
time the pod restarted, which would be a drag even for a minimal Kubernetes
setup).&lt;/li>
&lt;li>We set &lt;code>hostNetwork: true&lt;/code> which, unsurprisingly, sets up the etcd pod to use
the host network instead of the pod-internal network (this will make it easier
for the API server to find the etcd cluster).&lt;/li>
&lt;/ul>
&lt;p>Some quick sanity checks show that etcd is indeed listening on localhost and
writing to disk:&lt;/p>
&lt;div class="src src-text">
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-text" data-lang="text">&lt;span style="display:flex;">&lt;span>$ curl localhost:2379/version
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>{&amp;#34;etcdserver&amp;#34;:&amp;#34;3.4.3&amp;#34;,&amp;#34;etcdcluster&amp;#34;:&amp;#34;3.4.0&amp;#34;}
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>$ sudo tree /var/lib/etcd/
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>/var/lib/etcd/
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>└── member
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> ├── snap
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> │   └── db
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> └── wal
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> ├── 0.tmp
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> └── 0000000000000000-0000000000000000.wal&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>
&lt;/div>
&lt;/div>
&lt;/div>
&lt;div id="outline-container-headline-6" class="outline-2">
&lt;h2 id="headline-6">
Running the API server
&lt;/h2>
&lt;div id="outline-text-headline-6" class="outline-text-2">
&lt;p>
Getting the Kubernetes API server running is even easier. The only CLI flag we
have to pass is &lt;code>--etcd-servers&lt;/code>, which does what you&amp;#39;d expect:&lt;/p>
&lt;div class="src src-yaml">
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>&lt;span style="font-weight:bold">apiVersion&lt;/span>: v1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="font-weight:bold">kind&lt;/span>: Pod
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="font-weight:bold">metadata&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="font-weight:bold">name&lt;/span>: kube-apiserver
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="font-weight:bold">namespace&lt;/span>: kube-system
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="font-weight:bold">spec&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="font-weight:bold">containers&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - &lt;span style="font-weight:bold">name&lt;/span>: kube-apiserver
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="font-weight:bold">command&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - kube-apiserver
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - --etcd-servers=http://127.0.0.1:2379
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="font-weight:bold">image&lt;/span>: k8s.gcr.io/kube-apiserver:v1.18.5
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="font-weight:bold">hostNetwork&lt;/span>: &lt;span style="color:#fff;font-weight:bold">true&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>
&lt;/div>
&lt;p>
Put that YAML file in the &lt;code>pods&lt;/code> directory and the API server will start. Some
quick &lt;code>curl&lt;/code> ing shows that the Kubernetes API is listening on port 8080 with
completely open access—no authentication necessary!&lt;/p>
&lt;div class="src src-text">
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-text" data-lang="text">&lt;span style="display:flex;">&lt;span>$ curl localhost:8080/healthz
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>ok
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>$ curl localhost:8080/api/v1/pods
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>{
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &amp;#34;kind&amp;#34;: &amp;#34;PodList&amp;#34;,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &amp;#34;apiVersion&amp;#34;: &amp;#34;v1&amp;#34;,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &amp;#34;metadata&amp;#34;: {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &amp;#34;selfLink&amp;#34;: &amp;#34;/api/v1/pods&amp;#34;,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &amp;#34;resourceVersion&amp;#34;: &amp;#34;59&amp;#34;
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> },
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &amp;#34;items&amp;#34;: []
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>}&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>
&lt;/div>
&lt;p>
(Again, don&amp;#39;t run this setup in production! I was a bit surprised that the
default setup is so insecure, but I assume it&amp;#39;s to make development and testing
easier.)&lt;/p>
&lt;p>
And, as a nice surprise, kubectl works out of the box with no extra
configuration!&lt;/p>
&lt;div class="src src-text">
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-text" data-lang="text">&lt;span style="display:flex;">&lt;span>$ ./kubectl version
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>Client Version: version.Info{Major:&amp;#34;1&amp;#34;, Minor:&amp;#34;18&amp;#34;, GitVersion:&amp;#34;v1.18.5&amp;#34;, GitCommit:&amp;#34;e6503f8d8f769ace2f338794c914a96fc335df0f&amp;#34;, GitTreeState:&amp;#34;clean&amp;#34;, BuildDate:&amp;#34;2020-06-26T03:47:41Z&amp;#34;, GoVersion:&amp;#34;go1.13.9&amp;#34;, Compiler:&amp;#34;gc&amp;#34;, Platform:&amp;#34;linux/amd64&amp;#34;}
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>Server Version: version.Info{Major:&amp;#34;1&amp;#34;, Minor:&amp;#34;18&amp;#34;, GitVersion:&amp;#34;v1.18.5&amp;#34;, GitCommit:&amp;#34;e6503f8d8f769ace2f338794c914a96fc335df0f&amp;#34;, GitTreeState:&amp;#34;clean&amp;#34;, BuildDate:&amp;#34;2020-06-26T03:39:24Z&amp;#34;, GoVersion:&amp;#34;go1.13.9&amp;#34;, Compiler:&amp;#34;gc&amp;#34;, Platform:&amp;#34;linux/amd64&amp;#34;}
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>$ ./kubectl get pod
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>No resources found in default namespace.&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>
&lt;/div>
&lt;p>
Easy, right?&lt;/p>
&lt;/div>
&lt;/div>
&lt;div id="outline-container-headline-7" class="outline-2">
&lt;h2 id="headline-7">
A Problem
&lt;/h2>
&lt;div id="outline-text-headline-7" class="outline-text-2">
&lt;p>
But digging a bit deeper, something seems amiss:&lt;/p>
&lt;div class="src src-text">
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-text" data-lang="text">&lt;span style="display:flex;">&lt;span>$ ./kubectl get pod -n kube-system
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>No resources found in kube-system namespace.&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>
&lt;/div>
&lt;p>
Those static pods we set up earlier are missing! In fact, our kubelet-running
node isn&amp;#39;t showing up at all:&lt;/p>
&lt;div class="src src-text">
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-text" data-lang="text">&lt;span style="display:flex;">&lt;span>$ ./kubectl get nodes
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>No resources found in default namespace.&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>
&lt;/div>
&lt;p>
What&amp;#39;s the issue? Well, if you remember from a few paragraphs past, we&amp;#39;re
running kubelet with an extremely basic set of CLI flags, so kubelet doesn&amp;#39;t
know how to communicate with the API server and update it on its status. With
some perusing through the CLI documentation, we find the relevant flag:&lt;/p>
&lt;blockquote>
&lt;p>&lt;code>--kubeconfig&lt;/code> string&lt;/p>
&lt;p>
Path to a kubeconfig file, specifying how to connect to the API
server. Providing –kubeconfig enables API server mode, omitting –kubeconfig
enables standalone mode.&lt;/p>
&lt;/blockquote>
&lt;p>
This whole time we&amp;#39;ve been running kubelet in &amp;#34;standalone mode&amp;#34; without knowing
it. (If we were being pedantic we might have considered standalone kubelet to be
the &amp;#34;minimum viable Kubernetes setup&amp;#34;, but that would have made for a boring
blog post.) To get the &amp;#34;real&amp;#34; setup working we need to pass a kubeconfig file to
kubelet so that it knows how to talk to the API server. Thankfully that&amp;#39;s pretty
easy (since we have no authentication or certificates to worry about):&lt;/p>
&lt;div class="src src-yaml">
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>&lt;span style="font-weight:bold">apiVersion&lt;/span>: v1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="font-weight:bold">kind&lt;/span>: Config
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="font-weight:bold">clusters&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>- &lt;span style="font-weight:bold">cluster&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="font-weight:bold">server&lt;/span>: http://127.0.0.1:8080
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="font-weight:bold">name&lt;/span>: mink8s
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="font-weight:bold">contexts&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>- &lt;span style="font-weight:bold">context&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="font-weight:bold">cluster&lt;/span>: mink8s
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="font-weight:bold">name&lt;/span>: mink8s
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="font-weight:bold">current-context&lt;/span>: mink8s&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>
&lt;/div>
&lt;p>
Save that as &lt;code>kubeconfig.yaml&lt;/code>, kill the kubelet process, and restart with the
necessary CLI flag:&lt;/p>
&lt;div class="src src-text">
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-text" data-lang="text">&lt;span style="display:flex;">&lt;span>$ sudo ./kubelet --pod-manifest-path=pods --kubeconfig=kubeconfig.yaml&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>
&lt;/div>
&lt;p>
(As an aside, if you try &lt;code>curl&lt;/code> ing the API while kubelet is dead, you&amp;#39;ll find
that it still works! Kubelet isn&amp;#39;t a &amp;#34;parent&amp;#34; of its pods in the way that Docker
is, it&amp;#39;s more like a &amp;#34;management daemon&amp;#34;. kubelet-managed containers will keep
running indefinitely until kubelet stops them.)&lt;/p>
&lt;p>
After a few minutes, kubectl should show us the pods and nodes as expected:&lt;/p>
&lt;div class="src src-text">
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-text" data-lang="text">&lt;span style="display:flex;">&lt;span>$ ./kubectl get pods -A
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>NAMESPACE NAME READY STATUS RESTARTS AGE
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>default hello-mink8s 0/1 CrashLoopBackOff 261 21h
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>kube-system etcd-mink8s 1/1 Running 0 21h
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>kube-system kube-apiserver-mink8s 1/1 Running 0 21h
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>$ ./kubectl get nodes -owide
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>NAME STATUS ROLES AGE VERSION INTERNAL-IP EXTERNAL-IP OS-IMAGE KERNEL-VERSION CONTAINER-RUNTIME
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>mink8s Ready &amp;lt;none&amp;gt; 21h v1.18.5 10.70.10.228 &amp;lt;none&amp;gt; Ubuntu 18.04.4 LTS 4.15.0-109-generic docker://19.3.6&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>
&lt;/div>
&lt;p>
Let&amp;#39;s congratulate ourselves for real this time (I know I did at this point)—we
have an extremely minimal Kubernetes &amp;#34;cluster&amp;#34; running with a fully-functioning
API!&lt;/p>
&lt;/div>
&lt;/div>
&lt;div id="outline-container-headline-8" class="outline-2">
&lt;h2 id="headline-8">
Getting a Pod Running
&lt;/h2>
&lt;div id="outline-text-headline-8" class="outline-text-2">
&lt;p>
Now let&amp;#39;s see what the API can do. We&amp;#39;ll start with the old standby, an nginx
pod:&lt;/p>
&lt;div class="src src-yaml">
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>&lt;span style="font-weight:bold">apiVersion&lt;/span>: v1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="font-weight:bold">kind&lt;/span>: Pod
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="font-weight:bold">metadata&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="font-weight:bold">name&lt;/span>: nginx
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="font-weight:bold">spec&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="font-weight:bold">containers&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - &lt;span style="font-weight:bold">image&lt;/span>: nginx
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="font-weight:bold">name&lt;/span>: nginx&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>
&lt;/div>
&lt;p>
When we try to apply it, we find a rather curious error:&lt;/p>
&lt;div class="src src-text">
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-text" data-lang="text">&lt;span style="display:flex;">&lt;span>$ ./kubectl apply -f nginx.yaml
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>Error from server (Forbidden): error when creating &amp;#34;nginx.yaml&amp;#34;: pods &amp;#34;nginx&amp;#34; is
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>forbidden: error looking up service account default/default: serviceaccount
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&amp;#34;default&amp;#34; not found
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>$ ./kubectl get serviceaccounts
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>No resources found in default namespace.&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>
&lt;/div>
&lt;p>
This is our first indication of how woefully incomplete our Kubernetes
environment is—there are no service accounts for our pods to use. Let&amp;#39;s try
again by making a default service account manually and see what happens:&lt;/p>
&lt;div class="src src-text">
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-text" data-lang="text">&lt;span style="display:flex;">&lt;span>$ cat &amp;lt;&amp;lt;EOS | ./kubectl apply -f -
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>apiVersion: v1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>kind: ServiceAccount
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>metadata:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> name: default
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> namespace: default
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>EOS
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>serviceaccount/default created
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>$ ./kubectl apply -f nginx.yaml
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>Error from server (ServerTimeout): error when creating &amp;#34;nginx.yaml&amp;#34;: No API
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>token found for service account &amp;#34;default&amp;#34;, retry after the token is
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>automatically created and added to the service account&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>
&lt;/div>
&lt;p>
Even when we make the service account manually, the authentication token never
gets created. As we continue using our minimal &amp;#34;cluster&amp;#34;, we&amp;#39;ll find that most
of the useful things that normally happen automatically will be missing. The
Kubernetes API server is pretty minimal; most of the heavy automatic lifting
happens in various controllers and background jobs that aren&amp;#39;t running yet.&lt;/p>
&lt;p>
We can sidestep this particular issue by setting the
&lt;code>automountServiceAccountToken&lt;/code> option on the service account (since we won&amp;#39;t be
needing to use the service account anyways):&lt;/p>
&lt;div class="src src-text">
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-text" data-lang="text">&lt;span style="display:flex;">&lt;span>$ cat &amp;lt;&amp;lt;EOS | ./kubectl apply -f -
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>apiVersion: v1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>kind: ServiceAccount
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>metadata:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> name: default
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> namespace: default
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>automountServiceAccountToken: false
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>EOS
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>serviceaccount/default configured
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>$ ./kubectl apply -f nginx.yaml
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>pod/nginx created
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>$ ./kubectl get pods
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>NAME READY STATUS RESTARTS AGE
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>nginx 0/1 Pending 0 13m&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>
&lt;/div>
&lt;p>
Finally, the pod shows up! But it won&amp;#39;t actually start since we&amp;#39;re missing &lt;a href="https://kubernetes.io/docs/concepts/scheduling-eviction/kube-scheduler/">the
scheduler&lt;/a>, another essential Kubernetes component. Again, we see that the
Kubernetes API is surprisingly &amp;#34;dumb&amp;#34;—when you create a pod in the API, it
registers its existence but doesn&amp;#39;t try to figure out which node to run it on.&lt;/p>
&lt;p>
But we don&amp;#39;t actually need the scheduler to get pods running. We can just add
the node manually to the manifest with the &lt;code>nodeName&lt;/code> option:&lt;/p>
&lt;div class="src src-yaml">
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>&lt;span style="font-weight:bold">apiVersion&lt;/span>: v1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="font-weight:bold">kind&lt;/span>: Pod
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="font-weight:bold">metadata&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="font-weight:bold">name&lt;/span>: nginx
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="font-weight:bold">spec&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="font-weight:bold">containers&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - &lt;span style="font-weight:bold">image&lt;/span>: nginx
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="font-weight:bold">name&lt;/span>: nginx
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="font-weight:bold">nodeName&lt;/span>: mink8s&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>
&lt;/div>
&lt;p>
(You&amp;#39;ll replace &lt;code>mink8s&lt;/code> with whatever the node is called.) After deleting the
old pod and reapplying, we see that nginx will start up and listen on an
internal IP address:&lt;/p>
&lt;div class="src src-text">
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-text" data-lang="text">&lt;span style="display:flex;">&lt;span>$ ./kubectl delete pod nginx
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>pod &amp;#34;nginx&amp;#34; deleted
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>$ ./kubectl apply -f nginx.yaml
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>pod/nginx created
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>$ ./kubectl get pods -owide
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>nginx 1/1 Running 0 30s 172.17.0.2 mink8s &amp;lt;none&amp;gt; &amp;lt;none&amp;gt;
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>$ curl -s 172.17.0.2 | head -4
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&amp;lt;!DOCTYPE html&amp;gt;
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&amp;lt;html&amp;gt;
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&amp;lt;head&amp;gt;
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&amp;lt;title&amp;gt;Welcome to nginx!&amp;lt;/title&amp;gt;&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>
&lt;/div>
&lt;p>
To confirm that pod-to-pod networking is working correctly, we can run curl from
a different pod:&lt;/p>
&lt;div class="src src-text">
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-text" data-lang="text">&lt;span style="display:flex;">&lt;span>$ cat &amp;lt;&amp;lt;EOS | ./kubectl apply -f -
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>apiVersion: v1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>kind: Pod
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>metadata:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> name: curl
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>spec:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> containers:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - image: curlimages/curl
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> name: curl
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> command: [&amp;#34;curl&amp;#34;, &amp;#34;172.17.0.2&amp;#34;]
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> nodeName: mink8s
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>EOS
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>pod/curl created
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>$ ./kubectl logs curl | head -6
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> % Total % Received % Xferd Average Speed Time Time Time Current
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> Dload Upload Total Spent Left Speed
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&amp;lt;!DOCTYPE html&amp;gt;
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&amp;lt;html&amp;gt;
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&amp;lt;head&amp;gt;
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&amp;lt;title&amp;gt;Welcome to nginx!&amp;lt;/title&amp;gt;&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>
&lt;/div>
&lt;p>
It&amp;#39;s fun to poke around this environment to see what works and what doesn&amp;#39;t. I
found that ConfigMaps and Secrets work as expected, but Services and Deployments
are pretty much a no-go for now.&lt;/p>
&lt;/div>
&lt;/div>
&lt;div id="outline-container-headline-9" class="outline-2">
&lt;h2 id="headline-9">
SUCCESS!
&lt;/h2>
&lt;div id="outline-text-headline-9" class="outline-text-2">
&lt;p>
This post is getting a bit long, so I&amp;#39;m going to declare victory and arbitrarily
state that this is the minimum viable setup that could reasonable be called
&amp;#34;Kubernetes&amp;#34;. To recap: with 4 binaries, 5 CLI flags, and a &amp;#34;mere&amp;#34; 45 lines of
YAML (not much by Kubernetes standards), we have a fair number of things
working:&lt;/p>
&lt;ul>
&lt;li>Pods can be managed using the normal Kubernetes API (with a few hacks)&lt;/li>
&lt;li>Public container images can be pulled and managed&lt;/li>
&lt;li>Pods are kept alive and automatically restarted&lt;/li>
&lt;li>Pod-to-pod networking within a single node works pretty much fine&lt;/li>
&lt;li>ConfigMaps, Secrets, and basic volume mounts work as expected&lt;/li>
&lt;/ul>
&lt;p>But most of what makes Kubernetes truly useful is still missing, for example:&lt;/p>
&lt;ul>
&lt;li>Pod scheduling&lt;/li>
&lt;li>Authentication/authorization&lt;/li>
&lt;li>Multiple nodes&lt;/li>
&lt;li>Networking through Services&lt;/li>
&lt;li>Cluster-internal DNS&lt;/li>
&lt;li>Controllers for service accounts, deployments, cloud provider integration, and
most of the other &amp;#34;goodies&amp;#34; that Kubernetes brings&lt;/li>
&lt;/ul>
&lt;p>So what have we actually set up? The Kubernetes API running by itself is really
just a &lt;em>platform for automating containers&lt;/em>. It doesn&amp;#39;t do much fancy—that&amp;#39;s the
job of the various controllers and operators that use the API—but it does
provide a consistent basis for automation. (In future posts I might dig into the
details of some of the other Kubernetes components.)&lt;/p>
&lt;/div>
&lt;/div></description></item><item><title>Hello World! 👋</title><link>https://eevans.co/blog/hello-world/</link><pubDate>Mon, 25 May 2020 00:00:00 -0700</pubDate><guid>https://eevans.co/blog/hello-world/</guid><description>&lt;p>Welcome to my new blog! In the near future I&amp;rsquo;ll be writing about DevOps, Kubernetes, and general
tech stuff that I find interesting. In the meantime, here are some blog posts I wrote
for &lt;a href="https://www.rainforestqa.com">my previous employer&lt;/a>:&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://www.rainforestqa.com/blog/2019-04-02-why-we-moved-from-heroku-to-google-kubernetes-engine">Why We Moved from Heroku to Google Kubernetes
Engine&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://www.rainforestqa.com/blog/2019-05-09-how-we-moved-from-heroku-to-google-kubernetes-engine">How We Moved from Heroku to Google Kubernetes
Engine&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://www.rainforestqa.com/blog/2016-06-07-implementing-continuous-delivery-how-we-ship-code-at-rainforest">How We Ship Code at Rainforest&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>About Me</title><link>https://eevans.co/about/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://eevans.co/about/</guid><description>&lt;p>Hi, I&amp;rsquo;m Emanuel! 👋 I&amp;rsquo;ve worked in tech for about 9 years in a variety of
software development, leadership, and consulting roles.&lt;/p>
&lt;p>Take a look at &lt;a href="https://eevans.co/blog">my blog&lt;/a> to get a sense of the kind of things I like to
work on and &lt;a href="https://www.linkedin.com/in/emanuelevans/">my LinkedIn profile&lt;/a> to
see some of my work history.&lt;/p></description></item><item><title>Contact</title><link>https://eevans.co/contact/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://eevans.co/contact/</guid><description>&lt;p>If you want to get in touch, drop me a line by &lt;a href="mailto:in@eevans.co">sending me an
email&lt;/a> or a &lt;a href="https://www.linkedin.com/in/emanuelevans/">LinkedIn
message&lt;/a>.&lt;/p></description></item><item><title>Privacy Policy</title><link>https://eevans.co/privacy-policy/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://eevans.co/privacy-policy/</guid><description>&lt;p>This site uses services from the following companies, which may collect
information according to their privacy policies:&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Cloudflare&lt;/strong>: &lt;a href="https://www.cloudflare.com/privacypolicy/">https://www.cloudflare.com/privacypolicy/&lt;/a>&lt;/li>
&lt;li>&lt;strong>Google&lt;/strong>: &lt;a href="https://policies.google.com/technologies/partner-sites">https://policies.google.com/technologies/partner-sites&lt;/a>&lt;/li>
&lt;/ul></description></item></channel></rss>